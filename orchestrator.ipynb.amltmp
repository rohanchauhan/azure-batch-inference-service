{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Lead Scoring Batch Inference Pipeline\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we [UCI Bank Marketing](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing) dataset for lead scoring. Lead classification is used to score leads based on the information we gather. It is different from lead \n",
        "qualification which is used to identify ideal customers. Lead scoring helps sales and marketing team focus their efforts on customers who are most likely to buy."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure Workspace\n",
        "We use mlfow for tracking the experiment and logging metrics and artifacts "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "from azureml.core import Workspace, Experiment\r\n",
        "import mlflow\r\n",
        "\r\n",
        "# Setup Azure Workspace\r\n",
        "ws = Workspace.from_config()\r\n",
        "experiment_name = 'leads-pyspark-train'\r\n",
        "experiment = Experiment(workspace=ws, name=experiment_name)\r\n",
        "\r\n",
        "# Start MLflow Experiment\r\n",
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\r\n",
        "mlflow.set_experiment(experiment_name)\r\n",
        "run = mlflow.start_run()\r\n",
        "\r\n",
        "# Get Spark session\r\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626006498190
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate and Upload Batch Data\n",
        "We use most of the data for training and testing the model. Rest of the data is used\n",
        "to create batches by creating csv files with 10 records each. The batches are stored \n",
        "in local storage in folder called `batch-data`.\n",
        "\n",
        "After storing the batches in `batch-data`, the folder is uploaded to blob storage \n",
        "using datastore. The dataset is registered as File Dataset. Later on, we can monitor\n",
        "data drift by creating base and target dataset."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "\r\n",
        "df = spark.read.csv(\r\n",
        "    path='data/bank-additional-full.csv',\r\n",
        "    header=\"true\",\r\n",
        "    inferSchema=\"true\",\r\n",
        "    sep=\";\")\r\n",
        "trainDF, testDF, batchDF = df.randomSplit([.7, .29, .01], seed=999)\r\n",
        "batchData = batchDF.toPandas()\r\n",
        "\r\n",
        "# Create a folder for storing generated batch data\r\n",
        "batch_folder = './batch-data'\r\n",
        "os.makedirs(batch_folder, exist_ok=True)\r\n",
        "print(\"Folder created!\")\r\n",
        "\r\n",
        "# Save each sample as a separate file\r\n",
        "print(\"Saving files...\")\r\n",
        "x = 0\r\n",
        "y = 10\r\n",
        "for i in range(int(batchDF.count()/10)):\r\n",
        "    filename = str(i+1) + '.csv'\r\n",
        "    writeData=batchData[x:y]\r\n",
        "    writeData.to_csv(os.path.join(batch_folder, filename), sep=\",\")\r\n",
        "    x+=10\r\n",
        "    y+=10\r\n",
        "\r\n",
        "print(\"files saved!\")\r\n",
        "\r\n",
        "# Upload the files to the default datastore\r\n",
        "print(\"Uploading files to datastore...\")\r\n",
        "default_ds = ws.get_default_datastore()\r\n",
        "default_ds.upload(src_dir=\"batch-data\", target_path=\"batch-data\", overwrite=True, show_progress=True)\r\n",
        "\r\n",
        "# Register a dataset for the input data\r\n",
        "batch_data_set = Dataset.File.from_files(path=(default_ds, 'batch-data/'), validate=False)\r\n",
        "try:\r\n",
        "    batch_data_set = batch_data_set.register(workspace=ws, \r\n",
        "                                             name='leads-batch-data',\r\n",
        "                                             description='batch data for Marketing Leads UCI',\r\n",
        "                                             create_new_version=True)\r\n",
        "except Exception as ex:\r\n",
        "    print(ex)\r\n",
        "\r\n",
        "print(\"Done!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder created!\n",
            "Saving files...\n",
            "files saved!\n",
            "Uploading files to datastore...\n",
            "Uploading an estimated of 40 files\n",
            "Uploading batch-data/1.csv\n",
            "Uploaded batch-data/1.csv, 1 files out of an estimated total of 40\n",
            "Uploading batch-data/10.csv\n",
            "Uploaded batch-data/10.csv, 2 files out of an estimated total of 40\n",
            "Uploading batch-data/11.csv\n",
            "Uploaded batch-data/11.csv, 3 files out of an estimated total of 40\n",
            "Uploading batch-data/12.csv\n",
            "Uploaded batch-data/12.csv, 4 files out of an estimated total of 40\n",
            "Uploading batch-data/13.csv\n",
            "Uploaded batch-data/13.csv, 5 files out of an estimated total of 40\n",
            "Uploading batch-data/14.csv\n",
            "Uploaded batch-data/14.csv, 6 files out of an estimated total of 40\n",
            "Uploading batch-data/15.csv\n",
            "Uploaded batch-data/15.csv, 7 files out of an estimated total of 40\n",
            "Uploading batch-data/16.csv\n",
            "Uploaded batch-data/16.csv, 8 files out of an estimated total of 40\n",
            "Uploading batch-data/17.csv\n",
            "Uploaded batch-data/17.csv, 9 files out of an estimated total of 40\n",
            "Uploading batch-data/18.csv\n",
            "Uploaded batch-data/18.csv, 10 files out of an estimated total of 40\n",
            "Uploading batch-data/19.csv\n",
            "Uploaded batch-data/19.csv, 11 files out of an estimated total of 40\n",
            "Uploading batch-data/2.csv\n",
            "Uploaded batch-data/2.csv, 12 files out of an estimated total of 40\n",
            "Uploading batch-data/20.csv\n",
            "Uploaded batch-data/20.csv, 13 files out of an estimated total of 40\n",
            "Uploading batch-data/21.csv\n",
            "Uploaded batch-data/21.csv, 14 files out of an estimated total of 40\n",
            "Uploading batch-data/22.csv\n",
            "Uploaded batch-data/22.csv, 15 files out of an estimated total of 40\n",
            "Uploading batch-data/23.csv\n",
            "Uploaded batch-data/23.csv, 16 files out of an estimated total of 40\n",
            "Uploading batch-data/24.csv\n",
            "Uploaded batch-data/24.csv, 17 files out of an estimated total of 40\n",
            "Uploading batch-data/25.csv\n",
            "Uploaded batch-data/25.csv, 18 files out of an estimated total of 40\n",
            "Uploading batch-data/26.csv\n",
            "Uploaded batch-data/26.csv, 19 files out of an estimated total of 40\n",
            "Uploading batch-data/27.csv\n",
            "Uploaded batch-data/27.csv, 20 files out of an estimated total of 40\n",
            "Uploading batch-data/28.csv\n",
            "Uploaded batch-data/28.csv, 21 files out of an estimated total of 40\n",
            "Uploading batch-data/29.csv\n",
            "Uploaded batch-data/29.csv, 22 files out of an estimated total of 40\n",
            "Uploading batch-data/3.csv\n",
            "Uploaded batch-data/3.csv, 23 files out of an estimated total of 40\n",
            "Uploading batch-data/30.csv\n",
            "Uploaded batch-data/30.csv, 24 files out of an estimated total of 40\n",
            "Uploading batch-data/31.csv\n",
            "Uploaded batch-data/31.csv, 25 files out of an estimated total of 40\n",
            "Uploading batch-data/32.csv\n",
            "Uploaded batch-data/32.csv, 26 files out of an estimated total of 40\n",
            "Uploading batch-data/33.csv\n",
            "Uploaded batch-data/33.csv, 27 files out of an estimated total of 40\n",
            "Uploading batch-data/34.csv\n",
            "Uploaded batch-data/34.csv, 28 files out of an estimated total of 40\n",
            "Uploading batch-data/35.csv\n",
            "Uploaded batch-data/35.csv, 29 files out of an estimated total of 40\n",
            "Uploading batch-data/36.csv\n",
            "Uploaded batch-data/36.csv, 30 files out of an estimated total of 40\n",
            "Uploading batch-data/37.csv\n",
            "Uploaded batch-data/37.csv, 31 files out of an estimated total of 40\n",
            "Uploading batch-data/38.csv\n",
            "Uploaded batch-data/38.csv, 32 files out of an estimated total of 40\n",
            "Uploading batch-data/39.csv\n",
            "Uploaded batch-data/39.csv, 33 files out of an estimated total of 40\n",
            "Uploading batch-data/4.csv\n",
            "Uploaded batch-data/4.csv, 34 files out of an estimated total of 40\n",
            "Uploading batch-data/40.csv\n",
            "Uploaded batch-data/40.csv, 35 files out of an estimated total of 40\n",
            "Uploading batch-data/5.csv\n",
            "Uploaded batch-data/5.csv, 36 files out of an estimated total of 40\n",
            "Uploading batch-data/6.csv\n",
            "Uploaded batch-data/6.csv, 37 files out of an estimated total of 40\n",
            "Uploading batch-data/7.csv\n",
            "Uploaded batch-data/7.csv, 38 files out of an estimated total of 40\n",
            "Uploading batch-data/8.csv\n",
            "Uploaded batch-data/8.csv, 39 files out of an estimated total of 40\n",
            "Uploading batch-data/9.csv\n",
            "Uploaded batch-data/9.csv, 40 files out of an estimated total of 40\n",
            "Uploaded 40 files\n",
            "Done!\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626006515711
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model\n",
        "There are categorical variables in the dataset which can be either converted into indices or one-hot encoding. Tree based work\n",
        "better with indicies. So, we have used R formula transformation for SVM and Logistic Regression. For tree based models, \n",
        "we have used only numberical columns and indices columns.\n",
        "\n",
        "A custom Transformer was used to rename columns and to be used along in pipeline. However, there were some problems in integrating\n",
        "the custom transformer class in scoring script due to which we shifted to normal rename operations with Dataframe.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A Transformer used in pipelines for renaming columns \n",
        "from pyspark.ml import Transformer\n",
        "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable  \n",
        "\n",
        "\n",
        "class ColumnRenamer(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
        "    '''\n",
        "    Renames the following columns in the dataframe: \n",
        "    employment variation rate\n",
        "    consumer price index\n",
        "    consumer confidence index \n",
        "    number of employees \n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(ColumnRenamer, self).__init__()\n",
        "        self.columnsToBeRenamed = {\n",
        "            'emp.var.rate':'emp_var_rate',\n",
        "            'cons.price.idx':'cons_price_idx',\n",
        "            'cons.conf.idx':'cons_conf_idx',\n",
        "            'nr.employed':'nr_employed'}\n",
        "\n",
        "    def _transform(self, df):\n",
        "        for key in self.columnsToBeRenamed.keys():\n",
        "            df = df.withColumnRenamed(key, self.columnsToBeRenamed[key])\n",
        "        return df    \n",
        "rename_columns = ColumnRenamer()\n",
        "\n",
        "\n",
        "# Uses R Formula for automatic conversion of categorical labels to 1 hot encoding\n",
        "from pyspark.ml.feature import RFormula\n",
        "rFormula = RFormula(formula=\"y ~ .\", featuresCol=\"features\", labelCol=\"label\", handleInvalid=\"skip\")\n",
        "\n",
        "# Uses String Indexer and Numeric Columns only for Tree Based Classifiers\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "example_df = rename_columns.transform(trainDF)\n",
        "categorialColumns = [colname for (colname, dataType) in example_df.dtypes if ((dataType==\"string\") and (colname!=\"y\"))]\n",
        "stringIndexer = StringIndexer(inputCols=categorialColumns, outputCols=[c + \"Index\" for c in categorialColumns])\n",
        "oheEncoder = OneHotEncoder(inputCols=stringIndexer.getOutputCols(), outputCols=[c + \"ohe\" for c in categorialColumns])\n",
        "label_stringIdx = StringIndexer(inputCol=\"y\", outputCol=\"label\")\n",
        "numericColumns = [colname for (colname, dataType) in example_df.dtypes if (dataType==\"int\" or dataType==\"float\" or dataType==\"double\")]\n",
        "assembledInputs = numericColumns + [c + \"Index\" for c in categorialColumns]\n",
        "vecAssembler = VectorAssembler(inputCols=assembledInputs, outputCol=\"features\")"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626006516287
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models are trained and stored in local path in folder called `model`. Then they are registered in AML workspace. We use Reciever Operating Characteristics\n",
        "(ROC) curve, Precision Recall curve and accuracy for evaluating the models.\n",
        "\n",
        "We log the models and metrics using MLflow."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, LinearSVC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "import mlflow.spark\n",
        "import pandas as pd\n",
        "\n",
        "# For Tracking Models\n",
        "model_num=1\n",
        "pipelineModel = None\n",
        "\n",
        "# Evaluators for performance metrics\n",
        "bevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\")\n",
        "mevaluator = MulticlassClassificationEvaluator()\n",
        "\n",
        "# Rename Columns\n",
        "columnsToBeRenamed = {'emp.var.rate':'emp_var_rate','cons.price.idx':'cons_price_idx','cons.conf.idx':'cons_conf_idx','nr.employed':'nr_employed'}\n",
        "for key in columnsToBeRenamed.keys():\n",
        "    trainDF = trainDF.withColumnRenamed(key, columnsToBeRenamed[key])\n",
        "for key in columnsToBeRenamed.keys():\n",
        "    testDF = testDF.withColumnRenamed(key, columnsToBeRenamed[key])\n",
        "\n",
        "# Non Tree Based Models\n",
        "non_tree_models = [LogisticRegression(), LinearSVC()]\n",
        "for model in non_tree_models:\n",
        "    non_tree_pipeline = Pipeline(stages=[rFormula, model])\n",
        "    pipelineModel = non_tree_pipeline.fit(trainDF)\n",
        "    predDF = pipelineModel.transform(testDF)\n",
        "\n",
        "    modelName =str(model_num)+'-'+model.__class__.__name__\n",
        "    accuracy = mevaluator.setMetricName(\"accuracy\").evaluate(predDF)\n",
        "    roc = bevaluator.setMetricName(\"areaUnderROC\").evaluate(predDF)\n",
        "    pr = bevaluator.setMetricName(\"areaUnderPR\").evaluate(predDF)\n",
        "    model_num += 1\n",
        "\n",
        "\n",
        "    # Log metrics and model\n",
        "    mlflow.spark.log_model(pipelineModel, modelName)\n",
        "    mlflow.log_metrics({\"modelNum\":model_num, \"accuracy\":accuracy, \"areaUnderROC\":roc, \"areaUnderPR\":pr})\n",
        "    print(\"Training complete:\",modelName)\n",
        "\n",
        "# Tree Based Models\n",
        "tree_models = [DecisionTreeClassifier(), RandomForestClassifier(), GBTClassifier()]\n",
        "for model in tree_models:\n",
        "    tree_pipeline = Pipeline(stages=[stringIndexer, oheEncoder, label_stringIdx, vecAssembler,model])\n",
        "    pipelineModel = tree_pipeline.fit(trainDF)\n",
        "    predDF = pipelineModel.transform(testDF)\n",
        "\n",
        "    modelName = str(model_num)+'-'+model.__class__.__name__\n",
        "    accuracy = mevaluator.setMetricName(\"accuracy\").evaluate(predDF)\n",
        "    roc = bevaluator.setMetricName(\"areaUnderROC\").evaluate(predDF)\n",
        "    pr = bevaluator.setMetricName(\"areaUnderPR\").evaluate(predDF)\n",
        "    model_num += 1\n",
        "\n",
        "    # Log metrics and model\n",
        "    mlflow.spark.log_model(pipelineModel, modelName)\n",
        "    mlflow.log_metrics({\"modelNum\":model_num, \"accuracy\":accuracy, \"areaUnderROC\":roc, \"areaUnderPR\":pr})\n",
        "    print(\"Training complete:\",modelName)\n",
        "\n",
        "\n",
        "pipelineModel.save('model')\n",
        "\n",
        "from azureml.core import Model\n",
        "Model.register(\n",
        "    workspace=ws,\n",
        "    model_path='model/',\n",
        "    model_name='pyspark-batch-leads-model',\n",
        ")\n",
        "\n",
        "mlflow.end_run()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete: 1-LogisticRegression\n",
            "Training complete: 2-LinearSVC\n",
            "Training complete: 3-DecisionTreeClassifier\n",
            "Training complete: 4-RandomForestClassifier\n",
            "Training complete: 5-GBTClassifier\n",
            "Registering model pyspark-batch-leads-model\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626006734924
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Compute\n",
        "We create compute which is to be used for running batch inference jobs."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "# Compute params\r\n",
        "compute_name = 'rohan-vm-cluster'\r\n",
        "inference_cluster = None\r\n",
        "\r\n",
        "if compute_name in ws.compute_targets:\r\n",
        "    inference_cluster = ComputeTarget(ws, compute_name)\r\n",
        "    print(\"Using existing cluster.\")\r\n",
        "else:\r\n",
        "    try:\r\n",
        "        compute_config = AmlCompute.provisioning_configuration(\r\n",
        "            vm_size ='STANDARD_DS11_V2', \r\n",
        "            max_nodes=2 )\r\n",
        "        inference_cluster = ComputeTarget.create(ws, compute_name, compute_config)\r\n",
        "        inference_cluster.wait_for_completion(show_output=True)\r\n",
        "    except Exception as ex:\r\n",
        "        print(ex)\r\n",
        "    print(\"Cluster created.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing cluster.\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626006737892
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scoring Script\n",
        "We create a folder called `batch-pipeline` to save `environment.yml` file and `scoring script` for the pipeline."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a folder for storing batch-pipeline files\n",
        "batch_folder = './batch-pipeline'\n",
        "os.makedirs(batch_folder, exist_ok=True)\n",
        "print(\"Folder created!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder created!\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626006738128
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 'batch-pipeline/batch_environment.yml'\n",
        "name: mlflow-environment\n",
        "channels:\n",
        "  - defaults\n",
        "  - anaconda\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.6\n",
        "  - scikit-learn\n",
        "  - pip\n",
        "  - pandas\n",
        "  - numpy\n",
        "  - openjdk\n",
        "  - pip:\n",
        "    - pyspark\n",
        "    - mlflow\n",
        "    - azureml-mlflow\n",
        "    - azureml-defaults"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing batch-pipeline/batch_environment.yml\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 'batch-pipeline/score.py'\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "from azureml.core import Model\r\n",
        "from pyspark.ml import PipelineModel\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "def init():\r\n",
        "    global model, columnsToBeRenamed, spark\r\n",
        "    spark = SparkSession.builder.getOrCreate()\r\n",
        "    model_path = Model.get_model_path('pyspark-batch-leads-model')\r\n",
        "    model= PipelineModel.load(model_path)\r\n",
        "    columnsToBeRenamed = {'emp.var.rate':'emp_var_rate','cons.price.idx':'cons_price_idx','cons.conf.idx':'cons_conf_idx','nr.employed':'nr_employed'}\r\n",
        "\r\n",
        "def run(mini_batch):\r\n",
        "    # This runs for each batch\r\n",
        "    resultList = []\r\n",
        "    # process each file in the batch\r\n",
        "    for f in mini_batch:\r\n",
        "        df = spark.read.csv(path=f,header=\"true\",inferSchema=\"true\",sep=\",\").drop('_c0')\r\n",
        "        for key in columnsToBeRenamed.keys():\r\n",
        "            df = df.withColumnRenamed(key, columnsToBeRenamed[key]) \r\n",
        "        prediction = model.transform(df).select('prediction').toPandas().prediction.map({0.0:\"no\",1.0:\"yes\"}).to_numpy()\r\n",
        "        resultList.append(\"{}: {}\".format(os.path.basename(f), prediction))\r\n",
        "    return resultList\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing batch-pipeline/score.py\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625970033791
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Pipeline\n",
        "We create a batch inference pipeline with ParallelRunStep. ParallelRunStep parallelizes batch operations."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\r\n",
        "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\r\n",
        "\r\n",
        "# Create an Environment for the experiment\r\n",
        "batch_env = Environment.from_conda_specification(name=\"experiment_env\", file_path=\"batch-pipeline/batch_environment.yml\")\r\n",
        "batch_env.docker.base_image = DEFAULT_CPU_IMAGE\r\n",
        "print('Configuration ready.')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration ready.\n"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626006738735
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\r\n",
        "from azureml.data import OutputFileDatasetConfig\r\n",
        "from azureml.core.runconfig import DockerConfiguration\r\n",
        "\r\n",
        "output_dir = OutputFileDatasetConfig(name='inferences')\r\n",
        "\r\n",
        "parallel_run_config = ParallelRunConfig(\r\n",
        "    source_directory='batch-pipeline/',\r\n",
        "    entry_script=\"score.py\",\r\n",
        "    mini_batch_size=\"5\",\r\n",
        "    error_threshold=10,\r\n",
        "    output_action=\"append_row\",\r\n",
        "    environment=batch_env,\r\n",
        "    compute_target=inference_cluster,\r\n",
        "    node_count=2)\r\n",
        "\r\n",
        "parallelrun_step = ParallelRunStep(\r\n",
        "    name='batch-score-leads',\r\n",
        "    parallel_run_config=parallel_run_config,\r\n",
        "    inputs=[batch_data_set.as_named_input('leads_batch')],\r\n",
        "    output=output_dir,\r\n",
        "    arguments=[],\r\n",
        "    allow_reuse=True\r\n",
        ")\r\n",
        "\r\n",
        "print('Steps defined')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps defined\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626006738923
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\r\n",
        "from azureml.pipeline.core import Pipeline\r\n",
        "\r\n",
        "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\r\n",
        "pipeline_run = Experiment(workspace=ws, name='leads-batch-pipeline').submit(pipeline)\r\n",
        "pipeline_run.wait_for_completion(show_output=True)\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created step batch-score-leads [9db6c1c1][20e22896-0b5a-40d4-9b58-15395e66c5c3], (This step will run and generate new outputs)\n",
            "Submitted PipelineRun 3ae02d0e-0e29-4a33-bff0-d2bf89a7bbca\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/3ae02d0e-0e29-4a33-bff0-d2bf89a7bbca?wsid=/subscriptions/23416925-66df-470c-b651-f378856d8ad7/resourcegroups/rohan-rg/workspaces/rohan-ws&tid=13715ad3-e049-4909-899b-f9e22f99b1a5\n",
            "PipelineRunId: 3ae02d0e-0e29-4a33-bff0-d2bf89a7bbca\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/3ae02d0e-0e29-4a33-bff0-d2bf89a7bbca?wsid=/subscriptions/23416925-66df-470c-b651-f378856d8ad7/resourcegroups/rohan-rg/workspaces/rohan-ws&tid=13715ad3-e049-4909-899b-f9e22f99b1a5\n",
            "PipelineRun Status: NotStarted\n",
            "PipelineRun Status: Running\n",
            "\n",
            "\n",
            "StepRunId: 2dab371b-5f8d-4d7e-a91c-c344634e8db6\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/2dab371b-5f8d-4d7e-a91c-c344634e8db6?wsid=/subscriptions/23416925-66df-470c-b651-f378856d8ad7/resourcegroups/rohan-rg/workspaces/rohan-ws&tid=13715ad3-e049-4909-899b-f9e22f99b1a5\n",
            "StepRun( batch-score-leads ) Status: NotStarted\n",
            "StepRun( batch-score-leads ) Status: Running\n",
            "\n",
            "Streaming azureml-logs/55_azureml-execution-tvmps_4328933f7688ddcadf0982e1cef70fc2817cec7cb999e175a29cd389a1d54ffa_d.txt\n",
            "========================================================================================================================\n",
            "2021-07-11T12:37:09Z Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/2dab371b-5f8d-4d7e-a91c-c344634e8db6/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/2dab371b-5f8d-4d7e-a91c-c344634e8db6/caches/workspaceblobstore -o ro --file-cache-timeout-in-seconds=1000000 --cache-size-mb=24766 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/2dab371b-5f8d-4d7e-a91c-c344634e8db6/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
            "2021-07-11T12:37:09Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/2dab371b-5f8d-4d7e-a91c-c344634e8db6/mounts/workspaceblobstore\n",
            "2021-07-11T12:37:09Z The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
            "2021-07-11T12:37:09Z Starting output-watcher...\n",
            "2021-07-11T12:37:09Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
            "2021-07-11T12:37:10Z Executing 'Copy ACR Details file' on 10.0.0.6\n",
            "2021-07-11T12:37:10Z Executing 'Copy ACR Details file' on 10.0.0.5\n",
            "2021-07-11T12:37:10Z Copy ACR Details file succeeded on 10.0.0.5. Output: \n",
            ">>>   \n",
            ">>>   \n",
            "2021-07-11T12:37:11Z Copy ACR Details file succeeded on 10.0.0.6. Output: \n",
            ">>>   \n",
            "Login Succeeded\n",
            "Using default tag: latest\n",
            "latest: Pulling from azureml/azureml_fe4afc798de401edfb76dc27a38b1703\n",
            "92473f7ef455: Pulling fs layer\n",
            "fb52bde70123: Pulling fs layer\n",
            "64788f86be3f: Pulling fs layer\n",
            "33f6d5f2e001: Pulling fs layer\n",
            "eeb715f1b6ae: Pulling fs layer\n",
            "fe519cf36537: Pulling fs layer\n",
            "58ff99196c15: Pulling fs layer\n",
            "9b13f06a8eff: Pulling fs layer\n",
            "2d4e93adbf58: Pulling fs layer\n",
            "6ee7c3767844: Pulling fs layer\n",
            "62cfc3ccb8ab: Pulling fs layer\n",
            "4a7af9d757ee: Pulling fs layer\n",
            "9e11d437728f: Pulling fs layer\n",
            "3506c910620f: Pulling fs layer\n",
            "afe6352c52c2: Pulling fs layer\n",
            "45d886309004: Pulling fs layer\n",
            "2ce19e789040: Pulling fs layer\n",
            "f2a2950e1ed4: Pulling fs layer\n",
            "6ee7c3767844: Waiting\n",
            "62cfc3ccb8ab: Waiting\n",
            "4a7af9d757ee: Waiting\n",
            "9e11d437728f: Waiting\n",
            "3506c910620f: Waiting\n",
            "afe6352c52c2: Waiting\n",
            "45d886309004: Waiting\n",
            "2ce19e789040: Waiting\n",
            "f2a2950e1ed4: Waiting\n",
            "33f6d5f2e001: Waiting\n",
            "eeb715f1b6ae: Waiting\n",
            "fe519cf36537: Waiting\n",
            "58ff99196c15: Waiting\n",
            "9b13f06a8eff: Waiting\n",
            "2d4e93adbf58: Waiting\n",
            "64788f86be3f: Verifying Checksum\n",
            "64788f86be3f: Download complete\n",
            "fb52bde70123: Download complete\n",
            "33f6d5f2e001: Verifying Checksum\n",
            "33f6d5f2e001: Download complete\n",
            "fe519cf36537: Verifying Checksum\n",
            "fe519cf36537: Download complete\n",
            "92473f7ef455: Verifying Checksum\n",
            "92473f7ef455: Download complete\n",
            "58ff99196c15: Verifying Checksum\n",
            "58ff99196c15: Download complete\n",
            "eeb715f1b6ae: Verifying Checksum\n",
            "eeb715f1b6ae: Download complete\n",
            "9b13f06a8eff: Verifying Checksum\n",
            "9b13f06a8eff: Download complete\n",
            "62cfc3ccb8ab: Verifying Checksum\n",
            "62cfc3ccb8ab: Download complete\n",
            "4a7af9d757ee: Verifying Checksum\n",
            "4a7af9d757ee: Download complete\n",
            "6ee7c3767844: Verifying Checksum\n",
            "6ee7c3767844: Download complete\n",
            "9e11d437728f: Verifying Checksum\n",
            "9e11d437728f: Download complete\n",
            "afe6352c52c2: Verifying Checksum\n",
            "afe6352c52c2: Download complete\n",
            "92473f7ef455: Pull complete\n",
            "45d886309004: Verifying Checksum\n",
            "45d886309004: Download complete\n",
            "fb52bde70123: Pull complete\n",
            "2d4e93adbf58: Verifying Checksum\n",
            "2d4e93adbf58: Download complete\n",
            "2ce19e789040: Verifying Checksum\n",
            "2ce19e789040: Download complete\n",
            "64788f86be3f: Pull complete\n",
            "f2a2950e1ed4: Verifying Checksum\n",
            "f2a2950e1ed4: Download complete\n",
            "33f6d5f2e001: Pull complete\n",
            "3506c910620f: Verifying Checksum\n",
            "3506c910620f: Download complete\n",
            "eeb715f1b6ae: Pull complete\n",
            "fe519cf36537: Pull complete\n",
            "58ff99196c15: Pull complete\n",
            "9b13f06a8eff: Pull complete\n",
            "\n",
            "Streaming azureml-logs/65_job_prep-tvmps_9ac3e9747427fdd20084ce811556a9f70b9259a2e9a8b9d19cd4bf983faa205b_d.txt\n",
            "===============================================================================================================\n",
            "[2021-07-11T12:37:38.831884] Entering job preparation.\n",
            "[2021-07-11T12:37:39.482733] Starting job preparation.\n",
            "[2021-07-11T12:37:39.483029] Extracting the control code.\n",
            "[2021-07-11T12:37:39.483548] Starting extract_project.\n",
            "[2021-07-11T12:37:39.483693] Starting to extract zip file.\n",
            "[2021-07-11T12:37:39.583029] Finished extracting zip file.\n",
            "[2021-07-11T12:37:39.585643] Using urllib.request Python 3.0 or later\n",
            "[2021-07-11T12:37:39.585678] Start fetching snapshots.\n",
            "[2021-07-11T12:37:39.585712] Start fetching snapshot.\n",
            "[2021-07-11T12:37:39.585721] Retrieving project from snapshot: 18dca3b0-328a-4fdf-b4f0-c49a285f8ce7\n",
            "Starting the daemon thread to refresh tokens in background for process with pid = 40\n",
            "[2021-07-11T12:37:39.945012] Finished fetching snapshot.\n",
            "[2021-07-11T12:37:39.945048] Start fetching snapshot.\n",
            "[2021-07-11T12:37:39.945126] Retrieving project from snapshot: d0c57f8d-9f67-42a2-8634-16a49782dd43\n",
            "[2021-07-11T12:37:46.824973] Finished fetching snapshot.\n",
            "[2021-07-11T12:37:46.825006] Finished fetching snapshots.\n",
            "[2021-07-11T12:37:46.825027] Finished extract_project.\n",
            "[2021-07-11T12:37:46.825177] Finished fetching and extracting the control code.\n",
            "[2021-07-11T12:37:46.831329] Start run_history_prep.\n",
            "[2021-07-11T12:37:46.837312] Job preparation is complete.\n",
            "[2021-07-11T12:37:46.837563] Entering Data Context Managers in Sidecar\n",
            "[2021-07-11T12:37:46.838294] Running Sidecar prep cmd...\n",
            "[2021-07-11T12:37:47.143734] INFO azureml.sidecar.sidecar: Received task: enter_contexts. Running on Linux at /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/2dab371b-5f8d-4d7e-a91c-c344634e8db6/wd/azureml/2dab371b-5f8d-4d7e-a91c-c344634e8db6\n",
            "[2021-07-11T12:37:47.144817] INFO azureml.sidecar.sidecar: Invoking \"enter_contexts\" task with Context Managers: {\"context_managers\": [\"Dataset:context_managers.Datasets\"]}\n",
            "\n",
            "Streaming azureml-logs/65_job_prep-tvmps_4328933f7688ddcadf0982e1cef70fc2817cec7cb999e175a29cd389a1d54ffa_d.txt\n",
            "===============================================================================================================\n",
            "[2021-07-11T12:37:40.174367] Entering job preparation.\n",
            "[2021-07-11T12:37:40.865905] Starting job preparation.\n",
            "[2021-07-11T12:37:40.865937] Extracting the control code.\n",
            "[2021-07-11T12:37:40.866325] Starting extract_project.\n",
            "[2021-07-11T12:37:40.866365] Starting to extract zip file.\n",
            "[2021-07-11T12:37:40.904203] Finished extracting zip file.\n",
            "[2021-07-11T12:37:40.906388] Using urllib.request Python 3.0 or later\n",
            "[2021-07-11T12:37:40.906423] Start fetching snapshots.\n",
            "[2021-07-11T12:37:40.906457] Start fetching snapshot.\n",
            "[2021-07-11T12:37:40.906470] Retrieving project from snapshot: 18dca3b0-328a-4fdf-b4f0-c49a285f8ce7\n",
            "Starting the daemon thread to refresh tokens in background for process with pid = 40\n",
            "[2021-07-11T12:37:41.229623] Finished fetching snapshot.\n",
            "[2021-07-11T12:37:41.229660] Start fetching snapshot.\n",
            "[2021-07-11T12:37:41.229777] Retrieving project from snapshot: d0c57f8d-9f67-42a2-8634-16a49782dd43\n",
            "[2021-07-11T12:37:49.100433] Finished fetching snapshot.\n",
            "[2021-07-11T12:37:49.100469] Finished fetching snapshots.\n",
            "[2021-07-11T12:37:49.100490] Finished extract_project.\n",
            "[2021-07-11T12:37:49.100714] Finished fetching and extracting the control code.\n",
            "[2021-07-11T12:37:49.106924] Start run_history_prep.\n",
            "[2021-07-11T12:37:49.112887] Job preparation is complete.\n",
            "[2021-07-11T12:37:49.113167] Entering Data Context Managers in Sidecar\n",
            "[2021-07-11T12:37:49.113949] Running Sidecar prep cmd...\n",
            "[2021-07-11T12:37:49.424803] INFO azureml.sidecar.sidecar: Received task: enter_contexts. Running on Linux at /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/2dab371b-5f8d-4d7e-a91c-c344634e8db6/wd/azureml/2dab371b-5f8d-4d7e-a91c-c344634e8db6\n",
            "[2021-07-11T12:37:49.425531] INFO azureml.sidecar.sidecar: Invoking \"enter_contexts\" task with Context Managers: {\"context_managers\": [\"Dataset:context_managers.Datasets\"]}\n",
            "Enter __enter__ of DatasetContextManager\n",
            "SDK version: azureml-core==1.28.0 azureml-dataprep==2.16.0. Session id: 533cf2b6-198a-44f1-b7ab-e33eaf06cee6. Run id: 2dab371b-5f8d-4d7e-a91c-c344634e8db6.\n",
            "Processing 'leads_batch'.\n",
            "Processing dataset FileDataset\n",
            "{\n",
            "  \"source\": [\n",
            "    \"('rohands', 'batch-data/')\"\n",
            "  ],\n",
            "  \"definition\": [\n",
            "    \"GetDatastoreFiles\"\n",
            "  ],\n",
            "  \"registration\": {\n",
            "    \"id\": \"ac42d4b6-2efd-43e6-b87a-630a4598d1fe\",\n",
            "    \"name\": \"leads-batch-data\",\n",
            "    \"version\": 1,\n",
            "    \"description\": \"batch data for Marketing Leads UCI\",\n",
            "    \"workspace\": \"Workspace.create(name='rohan-ws', subscription_id='23416925-66df-470c-b651-f378856d8ad7', resource_group='rohan-rg')\"\n",
            "  }\n",
            "}\n",
            "Mounting leads_batch to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/2dab371b-5f8d-4d7e-a91c-c344634e8db6/wd/leads_batch_ac42d4b6-2efd-43e6-b87a-630a4598d1fe.\n",
            "Mounted leads_batch to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/2dab371b-5f8d-4d7e-a91c-c344634e8db6/wd/leads_batch_ac42d4b6-2efd-43e6-b87a-630a4598d1fe as folder.\n",
            "Processing 'inferences'.\n",
            "Mounted inferences to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/2dab371b-5f8d-4d7e-a91c-c344634e8db6/wd/inferences_rohands.\n",
            "Exit __enter__ of DatasetContextManager\n",
            "Set Dataset leads_batch's target path to /mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/2dab371b-5f8d-4d7e-a91c-c344634e8db6/wd/leads_batch_ac42d4b6-2efd-43e6-b87a-630a4598d1fe\n",
            "Set OutputDataset inferences's target path to /tmp/4c46dc90-7fd2-49e1-9407-93929abda240\n",
            "[2021-07-11T12:38:02.895911] INFO azureml.sidecar.task.enter_contexts: Entered Context Managers\n",
            "[2021-07-11T12:38:03.692770] Ran Sidecar prep cmd.\n",
            "[2021-07-11T12:38:03.692981] Running Context Managers in Sidecar complete.\n",
            "\n",
            "Streaming azureml-logs/70_driver_log.txt\n",
            "========================================\n",
            "2021/07/11 12:38:50 Starting App Insight Logger for task:  runTaskLet\n",
            "2021/07/11 12:38:50 Version: 3.0.01632.0003 Branch: .SourceBranch Commit: 4b96fb0\n",
            "2021/07/11 12:38:50 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/info\n",
            "2021/07/11 12:38:50 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/status\n",
            "[2021-07-11T12:38:50.929575] Entering context manager injector.\n",
            "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'Dataset:context_managers.Datasets', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['driver/amlbi_main.py', '--client_sdk_version', '1.32.0', '--scoring_module_name', 'score.py', '--mini_batch_size', '5', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--output', 'DatasetOutputConfig:inferences', '--input_fds_0', 'leads_batch'])\n",
            "Script type = None\n",
            "[2021-07-11T12:38:51.362187] Entering Run History Context Manager.\n",
            "[2021-07-11T12:38:54.362080] Current directory: /mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/2dab371b-5f8d-4d7e-a91c-c344634e8db6/wd/azureml/2dab371b-5f8d-4d7e-a91c-c344634e8db6\n",
            "[2021-07-11T12:38:54.362343] Preparing to call script [driver/amlbi_main.py] with arguments:['--client_sdk_version', '1.32.0', '--scoring_module_name', 'score.py', '--mini_batch_size', '5', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--output', '$inferences', '--input_fds_0', 'leads_batch']\n",
            "[2021-07-11T12:38:54.362372] After variable expansion, calling script [driver/amlbi_main.py] with arguments:['--client_sdk_version', '1.32.0', '--scoring_module_name', 'score.py', '--mini_batch_size', '5', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--output', '/tmp/4c46dc90-7fd2-49e1-9407-93929abda240', '--input_fds_0', 'leads_batch']\n",
            "\n",
            "2021/07/11 12:38:55 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
            "Stopped: false\n",
            "OriginalData: 1\n",
            "FilteredData: 0.\n",
            "\n",
            "Streaming azureml-logs/75_job_post-tvmps_4328933f7688ddcadf0982e1cef70fc2817cec7cb999e175a29cd389a1d54ffa_d.txt\n",
            "===============================================================================================================\n",
            "[2021-07-11T12:40:14.753792] Entering job release\n",
            "[2021-07-11T12:40:15.474947] Starting job release\n",
            "[2021-07-11T12:40:15.475404] Logging experiment finalizing status in history service.\n",
            "Starting the daemon thread to refresh tokens in background for process with pid = 790\n",
            "[2021-07-11T12:40:15.476179] job release stage : upload_datastore starting...\n",
            "[2021-07-11T12:40:15.476601] job release stage : start importing azureml.history._tracking in run_history_release.\n",
            "[2021-07-11T12:40:15.476737] job release stage : execute_job_release starting...\n",
            "[2021-07-11T12:40:15.488111] job release stage : copy_batchai_cached_logs starting...\n",
            "[2021-07-11T12:40:15.491641] job release stage : copy_batchai_cached_logs completed...[2021-07-11T12:40:15.491957] Entering context manager injector.\n",
            "\n",
            "[2021-07-11T12:40:15.503640] job release stage : upload_datastore completed...\n",
            "[2021-07-11T12:40:15.568035] job release stage : send_run_telemetry starting...\n",
            "[2021-07-11T12:40:15.580185] get vm size and vm region successfully.\n",
            "[2021-07-11T12:40:15.587138] get compute meta data successfully.\n",
            "[2021-07-11T12:40:15.609443] job release stage : execute_job_release completed...\n",
            "[2021-07-11T12:40:15.743045] post artifact meta request successfully.\n",
            "[2021-07-11T12:40:15.836353] upload compute record artifact successfully.\n",
            "[2021-07-11T12:40:15.836563] job release stage : send_run_telemetry completed...\n",
            "[2021-07-11T12:40:15.836944] Running in AzureML-Sidecar, starting to exit user context managers...\n",
            "[2021-07-11T12:40:15.837290] Running Sidecar release cmd...\n",
            "[2021-07-11T12:40:15.869509] INFO azureml.sidecar.sidecar: Received task: exit_contexts. Running on Linux at /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/2dab371b-5f8d-4d7e-a91c-c344634e8db6/wd/azureml/2dab371b-5f8d-4d7e-a91c-c344634e8db6\n",
            "Enter __exit__ of DatasetContextManager\n",
            "Unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/2dab371b-5f8d-4d7e-a91c-c344634e8db6/wd/leads_batch_ac42d4b6-2efd-43e6-b87a-630a4598d1fe.\n",
            "Finishing unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/2dab371b-5f8d-4d7e-a91c-c344634e8db6/wd/leads_batch_ac42d4b6-2efd-43e6-b87a-630a4598d1fe.\n",
            "Unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/2dab371b-5f8d-4d7e-a91c-c344634e8db6/wd/inferences_rohands.\n",
            "Finishing unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/2dab371b-5f8d-4d7e-a91c-c344634e8db6/wd/inferences_rohands.\n",
            "Exit __exit__ of DatasetContextManager\n",
            "[2021-07-11T12:40:15.908046] Removing absolute paths from host...\n",
            "[2021-07-11T12:40:15.908267] INFO azureml.sidecar.task.exit_contexts: Exited Context Managers\n",
            "[2021-07-11T12:40:16.347956] Ran Sidecar release cmd.\n",
            "[2021-07-11T12:40:16.348048] Job release is complete\n",
            "\n",
            "StepRun(batch-score-leads) Execution Summary\n",
            "=============================================\n",
            "StepRun( batch-score-leads ) Status: Finished\n",
            "{'runId': '2dab371b-5f8d-4d7e-a91c-c344634e8db6', 'target': 'rohan-vm-cluster', 'status': 'Completed', 'startTimeUtc': '2021-07-11T12:37:04.718154Z', 'endTimeUtc': '2021-07-11T12:40:30.762767Z', 'properties': {'ContentSnapshotId': '18dca3b0-328a-4fdf-b4f0-c49a285f8ce7', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': '20e22896-0b5a-40d4-9b58-15395e66c5c3', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': '9db6c1c1', 'azureml.pipelinerunid': '3ae02d0e-0e29-4a33-bff0-d2bf89a7bbca', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json', 'azureml.parallelrunstep': 'true'}, 'inputDatasets': [{'dataset': {'id': 'ac42d4b6-2efd-43e6-b87a-630a4598d1fe'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'leads_batch', 'mechanism': 'Mount'}}, {'dataset': {'id': 'dcaee28d-e0da-4573-a99a-c7439b815fa6'}, 'consumptionDetails': {'type': 'Reference'}}], 'outputDatasets': [{'identifier': {'savedId': 'dcaee28d-e0da-4573-a99a-c7439b815fa6'}, 'outputType': 'RunOutput', 'outputDetails': {'outputName': 'inferences'}, 'dataset': {\n",
            "  \"source\": [\n",
            "    \"('rohands', 'dataset/2dab371b-5f8d-4d7e-a91c-c344634e8db6/inferences/')\"\n",
            "  ],\n",
            "  \"definition\": [\n",
            "    \"GetDatastoreFiles\"\n",
            "  ],\n",
            "  \"registration\": {\n",
            "    \"id\": \"dcaee28d-e0da-4573-a99a-c7439b815fa6\",\n",
            "    \"name\": null,\n",
            "    \"version\": null,\n",
            "    \"workspace\": \"Workspace.create(name='rohan-ws', subscription_id='23416925-66df-470c-b651-f378856d8ad7', resource_group='rohan-rg')\"\n",
            "  }\n",
            "}}], 'runDefinition': {'script': 'driver/amlbi_main.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--client_sdk_version', '1.32.0', '--scoring_module_name', 'score.py', '--mini_batch_size', '5', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--output', 'DatasetOutputConfig:inferences', '--input_fds_0', 'leads_batch'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'rohan-vm-cluster', 'dataReferences': {}, 'data': {'leads_batch': {'dataLocation': {'dataset': {'id': 'ac42d4b6-2efd-43e6-b87a-630a4598d1fe', 'name': None, 'version': '1'}, 'dataPath': None, 'uri': None}, 'mechanism': 'Mount', 'environmentVariableName': 'leads_batch', 'pathOnCompute': None, 'overwrite': False}}, 'outputData': {'inferences': {'outputLocation': {'dataset': None, 'dataPath': {'datastoreName': 'rohands', 'relativePath': None}, 'uri': None}, 'mechanism': 'Mount', 'additionalOptions': {'pathOnCompute': '/tmp/4c46dc90-7fd2-49e1-9407-93929abda240/', 'registrationOptions': {'name': None, 'description': None, 'tags': None, 'datasetRegistrationOptions': {'additionalTransformation': None}}, 'uploadOptions': {'overwrite': False, 'sourceGlobs': {'globPatterns': None}}, 'mountOptions': None}}}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 2, 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'experiment_env', 'version': 'Autosave_2021-07-11T05:41:00Z_d8e992ec', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['defaults', 'anaconda', 'conda-forge'], 'dependencies': ['python=3.6', 'scikit-learn', 'pip', 'pandas', 'numpy', 'openjdk', {'pip': ['pyspark', 'mlflow', 'azureml-mlflow', 'azureml-defaults']}], 'name': 'azureml_f2739031ae37a487a0872ec6d8eff353'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210615.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': None, 'imageVersion': None, 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_4328933f7688ddcadf0982e1cef70fc2817cec7cb999e175a29cd389a1d54ffa_d.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/azureml-logs/55_azureml-execution-tvmps_4328933f7688ddcadf0982e1cef70fc2817cec7cb999e175a29cd389a1d54ffa_d.txt?sv=2019-02-02&sr=b&sig=jbG4ILQR4EkBg5wEIzzf%2BV%2BCv7mVMe01r7VxN944ieY%3D&st=2021-07-11T12%3A30%3A20Z&se=2021-07-11T20%3A40%3A20Z&sp=r', 'azureml-logs/55_azureml-execution-tvmps_9ac3e9747427fdd20084ce811556a9f70b9259a2e9a8b9d19cd4bf983faa205b_d.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/azureml-logs/55_azureml-execution-tvmps_9ac3e9747427fdd20084ce811556a9f70b9259a2e9a8b9d19cd4bf983faa205b_d.txt?sv=2019-02-02&sr=b&sig=Cb4xgHW4ZbZV%2FIrqgfY7Ik68A88Hz15zx8EVEm%2Frkzw%3D&st=2021-07-11T12%3A30%3A20Z&se=2021-07-11T20%3A40%3A20Z&sp=r', 'azureml-logs/65_job_prep-tvmps_4328933f7688ddcadf0982e1cef70fc2817cec7cb999e175a29cd389a1d54ffa_d.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/azureml-logs/65_job_prep-tvmps_4328933f7688ddcadf0982e1cef70fc2817cec7cb999e175a29cd389a1d54ffa_d.txt?sv=2019-02-02&sr=b&sig=vKDV19WO0DokTpcj3QL9xXTaTxbRxu%2BgJCikiomiEgc%3D&st=2021-07-11T12%3A30%3A20Z&se=2021-07-11T20%3A40%3A20Z&sp=r', 'azureml-logs/65_job_prep-tvmps_9ac3e9747427fdd20084ce811556a9f70b9259a2e9a8b9d19cd4bf983faa205b_d.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/azureml-logs/65_job_prep-tvmps_9ac3e9747427fdd20084ce811556a9f70b9259a2e9a8b9d19cd4bf983faa205b_d.txt?sv=2019-02-02&sr=b&sig=UgVLi8b576r%2Fw20ljYs0z%2BoKbPSJ3xPyMVZWcQQo7rs%3D&st=2021-07-11T12%3A30%3A20Z&se=2021-07-11T20%3A40%3A20Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=eN3lm4Ovf%2BIPEP3srwkOJARExoO8XL%2BYisz7%2BULys%2Bw%3D&st=2021-07-11T12%3A30%3A20Z&se=2021-07-11T20%3A40%3A20Z&sp=r', 'azureml-logs/75_job_post-tvmps_4328933f7688ddcadf0982e1cef70fc2817cec7cb999e175a29cd389a1d54ffa_d.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/azureml-logs/75_job_post-tvmps_4328933f7688ddcadf0982e1cef70fc2817cec7cb999e175a29cd389a1d54ffa_d.txt?sv=2019-02-02&sr=b&sig=N3IhrJc1Ih%2F7qU9LbIyu3eg9JnBoSMS9XaJRPCv7HgQ%3D&st=2021-07-11T12%3A30%3A20Z&se=2021-07-11T20%3A40%3A20Z&sp=r', 'azureml-logs/75_job_post-tvmps_9ac3e9747427fdd20084ce811556a9f70b9259a2e9a8b9d19cd4bf983faa205b_d.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/azureml-logs/75_job_post-tvmps_9ac3e9747427fdd20084ce811556a9f70b9259a2e9a8b9d19cd4bf983faa205b_d.txt?sv=2019-02-02&sr=b&sig=eO%2B%2BRmRIRteODNtWYmLwEhnF5lTz1t6EfgASOGQNAsk%3D&st=2021-07-11T12%3A30%3A20Z&se=2021-07-11T20%3A40%3A20Z&sp=r', 'azureml-logs/process_info.json': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=%2F4Bc%2FM%2BCUJSCKpn%2B3P9%2FEHC%2FH6HFW1feeM2uYZ36u3g%3D&st=2021-07-11T12%3A30%3A20Z&se=2021-07-11T20%3A40%3A20Z&sp=r', 'azureml-logs/process_status.json': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=L3dLq7NPvsmIDKYOolJXhtLofiC0l6moqCw1yaZKoLE%3D&st=2021-07-11T12%3A30%3A20Z&se=2021-07-11T20%3A40%3A20Z&sp=r', 'logs/azureml/102_azureml.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/logs/azureml/102_azureml.log?sv=2019-02-02&sr=b&sig=xxFJb%2FhXq4qx9%2FQeG61DR72lUfdko7OQve7g8kBi3IY%3D&st=2021-07-11T12%3A30%3A17Z&se=2021-07-11T20%3A40%3A17Z&sp=r', 'logs/azureml/88_azureml.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/logs/azureml/88_azureml.log?sv=2019-02-02&sr=b&sig=cxvpjuJD7s7h0rybFqTZ5ubkTP7YFy7%2FCnM0bAh8Dfk%3D&st=2021-07-11T12%3A30%3A17Z&se=2021-07-11T20%3A40%3A17Z&sp=r', 'logs/azureml/dataprep/backgroundProcess.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/logs/azureml/dataprep/backgroundProcess.log?sv=2019-02-02&sr=b&sig=lZCVqQfxIF0swKDoFjYO2cdpoqiPAdj7BqFSEEj7FxY%3D&st=2021-07-11T12%3A30%3A17Z&se=2021-07-11T20%3A40%3A17Z&sp=r', 'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-02-02&sr=b&sig=Te6pnNxXqzvLDkBwSxHTtTHqCMY0Ckain2TvWZ%2BmH6A%3D&st=2021-07-11T12%3A30%3A17Z&se=2021-07-11T20%3A40%3A17Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=DwJ%2FJ7KJSU7bdPqW4ptTvDSYPBr8IyK5V7DT3ZYjNQc%3D&st=2021-07-11T12%3A30%3A17Z&se=2021-07-11T20%3A40%3A17Z&sp=r', 'logs/azureml/job_prep_azureml.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=ZHoCxGNy94LvnV%2BWAEAUoUcB1XbL7WN3bgOmUMKmTBU%3D&st=2021-07-11T12%3A30%3A17Z&se=2021-07-11T20%3A40%3A17Z&sp=r', 'logs/azureml/job_release_azureml.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=u14WotFsLRsA%2FSk7h1VK4gNJF6dmaP3itzGcKzDXS0Q%3D&st=2021-07-11T12%3A30%3A17Z&se=2021-07-11T20%3A40%3A17Z&sp=r', 'logs/azureml/sidecar/tvmps_4328933f7688ddcadf0982e1cef70fc2817cec7cb999e175a29cd389a1d54ffa_d/all.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/logs/azureml/sidecar/tvmps_4328933f7688ddcadf0982e1cef70fc2817cec7cb999e175a29cd389a1d54ffa_d/all.log?sv=2019-02-02&sr=b&sig=w5eX%2FXFT19DZiToNxVunoOaH%2BKfG0n2z3sU1v%2FzeVko%3D&st=2021-07-11T12%3A30%3A17Z&se=2021-07-11T20%3A40%3A17Z&sp=r', 'logs/azureml/sidecar/tvmps_4328933f7688ddcadf0982e1cef70fc2817cec7cb999e175a29cd389a1d54ffa_d/task.enter_contexts.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/logs/azureml/sidecar/tvmps_4328933f7688ddcadf0982e1cef70fc2817cec7cb999e175a29cd389a1d54ffa_d/task.enter_contexts.log?sv=2019-02-02&sr=b&sig=jcNGd8taKdoeLmIq8IMB5T5X%2B4AIL3WIHG3bH7Wt9qA%3D&st=2021-07-11T12%3A30%3A17Z&se=2021-07-11T20%3A40%3A17Z&sp=r', 'logs/azureml/sidecar/tvmps_4328933f7688ddcadf0982e1cef70fc2817cec7cb999e175a29cd389a1d54ffa_d/task.exit_contexts.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/logs/azureml/sidecar/tvmps_4328933f7688ddcadf0982e1cef70fc2817cec7cb999e175a29cd389a1d54ffa_d/task.exit_contexts.log?sv=2019-02-02&sr=b&sig=6uEx4%2FynCnRAujdLhirUHtmDlBu0LDfytrd%2F8h9HCeE%3D&st=2021-07-11T12%3A30%3A17Z&se=2021-07-11T20%3A40%3A17Z&sp=r', 'logs/azureml/sidecar/tvmps_9ac3e9747427fdd20084ce811556a9f70b9259a2e9a8b9d19cd4bf983faa205b_d/all.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/logs/azureml/sidecar/tvmps_9ac3e9747427fdd20084ce811556a9f70b9259a2e9a8b9d19cd4bf983faa205b_d/all.log?sv=2019-02-02&sr=b&sig=DDizRDXxnDJZHt0ltjvEnBsEuYuCwbMATwyV9XHBKz0%3D&st=2021-07-11T12%3A30%3A17Z&se=2021-07-11T20%3A40%3A17Z&sp=r', 'logs/azureml/sidecar/tvmps_9ac3e9747427fdd20084ce811556a9f70b9259a2e9a8b9d19cd4bf983faa205b_d/task.enter_contexts.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/logs/azureml/sidecar/tvmps_9ac3e9747427fdd20084ce811556a9f70b9259a2e9a8b9d19cd4bf983faa205b_d/task.enter_contexts.log?sv=2019-02-02&sr=b&sig=5jMoYize4pPT6GiFYDBlZXdS6TOcIiNGECQFYqSUSK0%3D&st=2021-07-11T12%3A30%3A17Z&se=2021-07-11T20%3A40%3A17Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=5hy%2FKWBffHuDfC3b9eNnLzJNrZxKQYy%2FWpL6EmxpgAw%3D&st=2021-07-11T12%3A30%3A17Z&se=2021-07-11T20%3A40%3A17Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.2dab371b-5f8d-4d7e-a91c-c344634e8db6/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=FHmQPFJG4aGlHDZBKKLymK4esBXvIk9ColFutU2FFh4%3D&st=2021-07-11T12%3A30%3A17Z&se=2021-07-11T20%3A40%3A17Z&sp=r'}, 'submittedBy': 'Aishwarya Singh'}\n",
            "\n",
            "\n",
            "\n",
            "PipelineRun Execution Summary\n",
            "==============================\n",
            "PipelineRun Status: Finished\n",
            "{'runId': '3ae02d0e-0e29-4a33-bff0-d2bf89a7bbca', 'status': 'Completed', 'startTimeUtc': '2021-07-11T12:32:37.498911Z', 'endTimeUtc': '2021-07-11T12:40:40.676345Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.3ae02d0e-0e29-4a33-bff0-d2bf89a7bbca/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=Ez7kNKScG0cw9ROwlidS3DXh4kQBP1JV2jRk4pzCL0s%3D&st=2021-07-11T12%3A30%3A43Z&se=2021-07-11T20%3A40%3A43Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.3ae02d0e-0e29-4a33-bff0-d2bf89a7bbca/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=lnrC2NkT%2BjgZpw2pQLoYFlW3wHBpJGrPVTzbVPSyRH4%3D&st=2021-07-11T12%3A30%3A43Z&se=2021-07-11T20%3A40%3A43Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.3ae02d0e-0e29-4a33-bff0-d2bf89a7bbca/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=Yf%2FwAb%2FWgF1gy9p9TFJx2nBBNlpwfg5GDJHvbQ10wjE%3D&st=2021-07-11T12%3A30%3A43Z&se=2021-07-11T20%3A40%3A43Z&sp=r'}, 'submittedBy': 'Aishwarya Singh'}\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "'Finished'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626007243655
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch predictions from run\n",
        "A batch inference pipeline coelsce data from multiple nodes in cluster to a single file called `inference` in the pipeline's output.\n",
        "\n",
        "The result of batch inference pipeline is downloaded from the blob storage to local storage\n",
        "in `leads-results` folder. We look for the file `parallel_run_step.txt` in `leads-results` and load it as pandas dataframe.\n",
        "Later on, we write the final leads as csv in `leads-results` folder."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "# Remove the local results folder if left over from a previous run\n",
        "result_file = None\n",
        "shutil.rmtree('leads-results', ignore_errors=True)\n",
        "\n",
        "# Get the run for the first step and download its output\n",
        "prediction_run = next(pipeline_run.get_children())\n",
        "prediction_output = prediction_run.get_output_data('inferences')\n",
        "prediction_output.download(local_path='leads-results')\n",
        "\n",
        "# Traverse the folder hierarchy and find the results file\n",
        "for root, dirs, files in os.walk('leads-results'):\n",
        "    for file in files:\n",
        "        if file.endswith('parallel_run_step.txt'):\n",
        "            result_file = os.path.join(root,file)\n",
        "\n",
        "# cleanup output format\n",
        "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
        "df.columns = [\"File\", \"Prediction\"]\n",
        "df.to_csv(\"leads-results/leads.csv\", header=True,index=False)\n",
        "# Display the first 20 results\n",
        "df.head(20)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "      File                                         Prediction\n0    1.csv   ['no' 'no' 'yes' 'no' 'no' 'yes' 'no' 'no' 'n...\n1   10.csv   ['no' 'yes' 'no' 'no' 'no' 'no' 'no' 'no' 'no...\n2   11.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...\n3   12.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...\n4   13.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...\n5   14.csv   ['no' 'yes' 'no' 'no' 'no' 'no' 'no' 'no' 'no...\n6   15.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...\n7   16.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...\n8   17.csv   ['yes' 'no' 'no' 'yes' 'no' 'no' 'no' 'no' 'n...\n9   18.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...\n10  19.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'yes' 'no' 'no...\n11   2.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'yes...\n12  20.csv   ['yes' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no...\n13  21.csv   ['no' 'no' 'no' 'yes' 'no' 'no' 'no' 'no' 'no...\n14  22.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...\n15  23.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...\n16  24.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...\n17  25.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...\n18  26.csv   ['no' 'no' 'no' 'yes' 'no' 'no' 'no' 'no' 'no...\n19  27.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>File</th>\n      <th>Prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.csv</td>\n      <td>['no' 'no' 'yes' 'no' 'no' 'yes' 'no' 'no' 'n...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10.csv</td>\n      <td>['no' 'yes' 'no' 'no' 'no' 'no' 'no' 'no' 'no...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>12.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>14.csv</td>\n      <td>['no' 'yes' 'no' 'no' 'no' 'no' 'no' 'no' 'no...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>15.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>16.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>17.csv</td>\n      <td>['yes' 'no' 'no' 'yes' 'no' 'no' 'no' 'no' 'n...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>18.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>19.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'yes' 'no' 'no...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'yes...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>20.csv</td>\n      <td>['yes' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>21.csv</td>\n      <td>['no' 'no' 'no' 'yes' 'no' 'no' 'no' 'no' 'no...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>22.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>23.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>24.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>25.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>26.csv</td>\n      <td>['no' 'no' 'no' 'yes' 'no' 'no' 'no' 'no' 'no...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>27.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626007245381
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Publish Pipeline\n",
        "The pipeline is published online so that it can be consumed by user with the help of authentication toke."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "published_pipeline = pipeline_run.publish_pipeline(name='leads-batch-pipeline', description='Batch scoring of leads data from UCI', version='1.0')\r\n",
        "rest_endpoint = published_pipeline.endpoint\r\n",
        "print(rest_endpoint)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://centralindia.api.azureml.ms/pipelines/v1.0/subscriptions/23416925-66df-470c-b651-f378856d8ad7/resourceGroups/rohan-rg/providers/Microsoft.MachineLearningServices/workspaces/rohan-ws/PipelineRuns/PipelineSubmit/eedecb53-a277-43d7-a5ef-f31e71dba0b0\n"
          ]
        }
      ],
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626007248392
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the Pipeline\n",
        "We use HTTP request to run the deployed pipeline."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.authentication import InteractiveLoginAuthentication\r\n",
        "from azureml.pipeline.core.run import PipelineRun\r\n",
        "from azureml.widgets import RunDetails\r\n",
        "import requests\r\n",
        "\r\n",
        "interactive_auth = InteractiveLoginAuthentication()\r\n",
        "auth_header = interactive_auth.get_authentication_header()\r\n",
        "\r\n",
        "rest_endpoint = published_pipeline.endpoint\r\n",
        "response = requests.post(rest_endpoint, headers=auth_header, json={\"ExperimentName\": \"leads-batch-pipeline\"})\r\n",
        "run_id = response.json()[\"Id\"]\r\n",
        "\r\n",
        "published_pipeline_run = PipelineRun(ws.experiments['leads-batch-pipeline'], run_id)\r\n",
        "published_pipeline_run.wait_for_completion(show_output=False)\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PipelineRunId: 57ef8be9-11d7-43dd-90e0-64ab19d9ce41\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/57ef8be9-11d7-43dd-90e0-64ab19d9ce41?wsid=/subscriptions/23416925-66df-470c-b651-f378856d8ad7/resourcegroups/rohan-rg/workspaces/rohan-ws&tid=13715ad3-e049-4909-899b-f9e22f99b1a5\n",
            "{'runId': '57ef8be9-11d7-43dd-90e0-64ab19d9ce41', 'status': 'Completed', 'startTimeUtc': '2021-07-11T12:42:10.380337Z', 'endTimeUtc': '2021-07-11T12:42:13.077887Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'Unavailable', 'runType': 'HTTP', 'azureml.parameters': '{}', 'azureml.pipelineid': 'eedecb53-a277-43d7-a5ef-f31e71dba0b0'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.57ef8be9-11d7-43dd-90e0-64ab19d9ce41/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=gljhr1hQwcbwyUiCgMr31VA1ly9RtE4P7Kng9AeoHTU%3D&st=2021-07-11T12%3A32%3A13Z&se=2021-07-11T20%3A42%3A13Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.57ef8be9-11d7-43dd-90e0-64ab19d9ce41/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=gbZuaVaAaDi2v04Ba2RycdwqBf2CQrpPQc8CSyF%2B9Ko%3D&st=2021-07-11T12%3A32%3A13Z&se=2021-07-11T20%3A42%3A13Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.57ef8be9-11d7-43dd-90e0-64ab19d9ce41/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=LSOdddFxCg1RdFeM2HAAZWG6vCWaj%2FbVFcmdil%2F23l4%3D&st=2021-07-11T12%3A32%3A13Z&se=2021-07-11T20%3A42%3A13Z&sp=r'}, 'submittedBy': 'Aishwarya Singh'}\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "'Finished'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1626007333631
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}