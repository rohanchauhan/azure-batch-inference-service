{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Configure Workspace"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "from azureml.core import Workspace, Experiment\r\n",
        "import mlflow\r\n",
        "\r\n",
        "# Setup Azure Workspace\r\n",
        "ws = Workspace.from_config()\r\n",
        "experiment_name = 'leads-pyspark-train'\r\n",
        "experiment = Experiment(workspace=ws, name=experiment_name)\r\n",
        "\r\n",
        "# Start MLflow Experiment\r\n",
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\r\n",
        "mlflow.set_experiment(experiment_name)\r\n",
        "run = mlflow.start_run()\r\n",
        "\r\n",
        "# Get default datastore\r\n",
        "default_ds = ws.get_default_datastore()\r\n",
        "\r\n",
        "# Get Spark session\r\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625974117242
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate and Upload Batch Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "\r\n",
        "df = spark.read.csv(\r\n",
        "    path='data/bank-additional-full.csv',\r\n",
        "    header=\"true\",\r\n",
        "    inferSchema=\"true\",\r\n",
        "    sep=\";\")\r\n",
        "trainDF, testDF, batchDF = df.randomSplit([.7, .29, .01], seed=999)\r\n",
        "batchData = batchDF.toPandas()\r\n",
        "\r\n",
        "# Create a folder\r\n",
        "batch_folder = './batch-data'\r\n",
        "os.makedirs(batch_folder, exist_ok=True)\r\n",
        "print(\"Folder created!\")\r\n",
        "\r\n",
        "# Save each sample as a separate file\r\n",
        "print(\"Saving files...\")\r\n",
        "x = 0\r\n",
        "y = 10\r\n",
        "for i in range(int(batchDF.count()/10)):\r\n",
        "    filename = str(i+1) + '.csv'\r\n",
        "    writeData=batchData[x:y]\r\n",
        "    writeData.to_csv(os.path.join(batch_folder, filename), sep=\",\")\r\n",
        "    x+=10\r\n",
        "    y+=10\r\n",
        "\r\n",
        "print(\"files saved!\")\r\n",
        "\r\n",
        "# Upload the files to the default datastore\r\n",
        "print(\"Uploading files to datastore...\")\r\n",
        "default_ds = ws.get_default_datastore()\r\n",
        "default_ds.upload(src_dir=\"batch-data\", target_path=\"batch-data\", overwrite=True, show_progress=True)\r\n",
        "\r\n",
        "# Register a dataset for the input data\r\n",
        "batch_data_set = Dataset.File.from_files(path=(default_ds, 'batch-data/'), validate=False)\r\n",
        "try:\r\n",
        "    batch_data_set = batch_data_set.register(workspace=ws, \r\n",
        "                                             name='leads-batch-data',\r\n",
        "                                             description='batch data for Marketing Leads UCI',\r\n",
        "                                             create_new_version=True)\r\n",
        "except Exception as ex:\r\n",
        "    print(ex)\r\n",
        "\r\n",
        "print(\"Done!\")\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder created!\n",
            "Saving files...\n",
            "files saved!\n",
            "Uploading files to datastore...\n",
            "Uploading an estimated of 40 files\n",
            "Uploading batch-data/1.csv\n",
            "Uploaded batch-data/1.csv, 1 files out of an estimated total of 40\n",
            "Uploading batch-data/10.csv\n",
            "Uploaded batch-data/10.csv, 2 files out of an estimated total of 40\n",
            "Uploading batch-data/11.csv\n",
            "Uploaded batch-data/11.csv, 3 files out of an estimated total of 40\n",
            "Uploading batch-data/12.csv\n",
            "Uploaded batch-data/12.csv, 4 files out of an estimated total of 40\n",
            "Uploading batch-data/13.csv\n",
            "Uploaded batch-data/13.csv, 5 files out of an estimated total of 40\n",
            "Uploading batch-data/14.csv\n",
            "Uploaded batch-data/14.csv, 6 files out of an estimated total of 40\n",
            "Uploading batch-data/15.csv\n",
            "Uploaded batch-data/15.csv, 7 files out of an estimated total of 40\n",
            "Uploading batch-data/16.csv\n",
            "Uploaded batch-data/16.csv, 8 files out of an estimated total of 40\n",
            "Uploading batch-data/17.csv\n",
            "Uploaded batch-data/17.csv, 9 files out of an estimated total of 40\n",
            "Uploading batch-data/18.csv\n",
            "Uploaded batch-data/18.csv, 10 files out of an estimated total of 40\n",
            "Uploading batch-data/19.csv\n",
            "Uploaded batch-data/19.csv, 11 files out of an estimated total of 40\n",
            "Uploading batch-data/2.csv\n",
            "Uploaded batch-data/2.csv, 12 files out of an estimated total of 40\n",
            "Uploading batch-data/20.csv\n",
            "Uploaded batch-data/20.csv, 13 files out of an estimated total of 40\n",
            "Uploading batch-data/21.csv\n",
            "Uploaded batch-data/21.csv, 14 files out of an estimated total of 40\n",
            "Uploading batch-data/22.csv\n",
            "Uploaded batch-data/22.csv, 15 files out of an estimated total of 40\n",
            "Uploading batch-data/23.csv\n",
            "Uploaded batch-data/23.csv, 16 files out of an estimated total of 40\n",
            "Uploading batch-data/24.csv\n",
            "Uploaded batch-data/24.csv, 17 files out of an estimated total of 40\n",
            "Uploading batch-data/25.csv\n",
            "Uploaded batch-data/25.csv, 18 files out of an estimated total of 40\n",
            "Uploading batch-data/26.csv\n",
            "Uploaded batch-data/26.csv, 19 files out of an estimated total of 40\n",
            "Uploading batch-data/27.csv\n",
            "Uploaded batch-data/27.csv, 20 files out of an estimated total of 40\n",
            "Uploading batch-data/28.csv\n",
            "Uploaded batch-data/28.csv, 21 files out of an estimated total of 40\n",
            "Uploading batch-data/29.csv\n",
            "Uploaded batch-data/29.csv, 22 files out of an estimated total of 40\n",
            "Uploading batch-data/3.csv\n",
            "Uploaded batch-data/3.csv, 23 files out of an estimated total of 40\n",
            "Uploading batch-data/30.csv\n",
            "Uploaded batch-data/30.csv, 24 files out of an estimated total of 40\n",
            "Uploading batch-data/31.csv\n",
            "Uploaded batch-data/31.csv, 25 files out of an estimated total of 40\n",
            "Uploading batch-data/32.csv\n",
            "Uploaded batch-data/32.csv, 26 files out of an estimated total of 40\n",
            "Uploading batch-data/33.csv\n",
            "Uploaded batch-data/33.csv, 27 files out of an estimated total of 40\n",
            "Uploading batch-data/34.csv\n",
            "Uploaded batch-data/34.csv, 28 files out of an estimated total of 40\n",
            "Uploading batch-data/35.csv\n",
            "Uploaded batch-data/35.csv, 29 files out of an estimated total of 40\n",
            "Uploading batch-data/36.csv\n",
            "Uploaded batch-data/36.csv, 30 files out of an estimated total of 40\n",
            "Uploading batch-data/37.csv\n",
            "Uploaded batch-data/37.csv, 31 files out of an estimated total of 40\n",
            "Uploading batch-data/38.csv\n",
            "Uploaded batch-data/38.csv, 32 files out of an estimated total of 40\n",
            "Uploading batch-data/39.csv\n",
            "Uploaded batch-data/39.csv, 33 files out of an estimated total of 40\n",
            "Uploading batch-data/4.csv\n",
            "Uploaded batch-data/4.csv, 34 files out of an estimated total of 40\n",
            "Uploading batch-data/40.csv\n",
            "Uploaded batch-data/40.csv, 35 files out of an estimated total of 40\n",
            "Uploading batch-data/5.csv\n",
            "Uploaded batch-data/5.csv, 36 files out of an estimated total of 40\n",
            "Uploading batch-data/6.csv\n",
            "Uploaded batch-data/6.csv, 37 files out of an estimated total of 40\n",
            "Uploading batch-data/7.csv\n",
            "Uploaded batch-data/7.csv, 38 files out of an estimated total of 40\n",
            "Uploading batch-data/8.csv\n",
            "Uploaded batch-data/8.csv, 39 files out of an estimated total of 40\n",
            "Uploading batch-data/9.csv\n",
            "Uploaded batch-data/9.csv, 40 files out of an estimated total of 40\n",
            "Uploaded 40 files\n",
            "Done!\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625974133088
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A Transformer used in pipelines for renaming columns \n",
        "from pyspark.ml import Transformer\n",
        "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable  \n",
        "\n",
        "class ColumnRenamer(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
        "    '''\n",
        "    Renames the following columns in the dataframe: \n",
        "    employment variation rate\n",
        "    consumer price index\n",
        "    consumer confidence index \n",
        "    number of employees \n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(ColumnRenamer, self).__init__()\n",
        "        self.columnsToBeRenamed = {\n",
        "            'emp.var.rate':'emp_var_rate',\n",
        "            'cons.price.idx':'cons_price_idx',\n",
        "            'cons.conf.idx':'cons_conf_idx',\n",
        "            'nr.employed':'nr_employed'}\n",
        "\n",
        "    def _transform(self, df):\n",
        "        for key in self.columnsToBeRenamed.keys():\n",
        "            df = df.withColumnRenamed(key, self.columnsToBeRenamed[key])\n",
        "        return df    \n",
        "rename_columns = ColumnRenamer()\n",
        "\n",
        "# Uses R Formula for automatic conversion of categorical labels to 1 hot encoding\n",
        "from pyspark.ml.feature import RFormula\n",
        "rFormula = RFormula(formula=\"y ~ .\", featuresCol=\"features\", labelCol=\"label\", handleInvalid=\"skip\")\n",
        "\n",
        "# Uses String Indexer and Numeric Columns only for Tree Based Classifiers\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "example_df = rename_columns.transform(trainDF)\n",
        "categorialColumns = [colname for (colname, dataType) in example_df.dtypes if ((dataType==\"string\") and (colname!=\"y\"))]\n",
        "stringIndexer = StringIndexer(inputCols=categorialColumns, outputCols=[c + \"Index\" for c in categorialColumns])\n",
        "oheEncoder = OneHotEncoder(inputCols=stringIndexer.getOutputCols(), outputCols=[c + \"ohe\" for c in categorialColumns])\n",
        "label_stringIdx = StringIndexer(inputCol=\"y\", outputCol=\"label\")\n",
        "numericColumns = [colname for (colname, dataType) in example_df.dtypes if (dataType==\"int\" or dataType==\"float\" or dataType==\"double\")]\n",
        "assembledInputs = numericColumns + [c + \"Index\" for c in categorialColumns]\n",
        "vecAssembler = VectorAssembler(inputCols=assembledInputs, outputCol=\"features\")"
      ],
      "outputs": [],
      "execution_count": 55,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625984383932
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, LinearSVC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "import mlflow.spark\n",
        "import pandas as pd\n",
        "\n",
        "# For Tracking Models\n",
        "model_num=1\n",
        "pipelineModel = None\n",
        "\n",
        "# Evaluators for performance metrics\n",
        "bevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\")\n",
        "mevaluator = MulticlassClassificationEvaluator()\n",
        "\n",
        "# Rename Columns\n",
        "columnsToBeRenamed = {'emp.var.rate':'emp_var_rate','cons.price.idx':'cons_price_idx','cons.conf.idx':'cons_conf_idx','nr.employed':'nr_employed'}\n",
        "for key in columnsToBeRenamed.keys():\n",
        "    trainDF = trainDF.withColumnRenamed(key, columnsToBeRenamed[key])\n",
        "for key in columnsToBeRenamed.keys():\n",
        "    testDF = testDF.withColumnRenamed(key, columnsToBeRenamed[key])\n",
        "\n",
        "# Non Tree Based Models\n",
        "non_tree_models = [LogisticRegression(), LinearSVC()]\n",
        "for model in non_tree_models:\n",
        "    non_tree_pipeline = Pipeline(stages=[rFormula, model])\n",
        "    pipelineModel = non_tree_pipeline.fit(trainDF)\n",
        "    predDF = pipelineModel.transform(testDF)\n",
        "\n",
        "    modelName =str(model_num)+'-'+model.__class__.__name__\n",
        "    accuracy = mevaluator.setMetricName(\"accuracy\").evaluate(predDF)\n",
        "    roc = bevaluator.setMetricName(\"areaUnderROC\").evaluate(predDF)\n",
        "    pr = bevaluator.setMetricName(\"areaUnderPR\").evaluate(predDF)\n",
        "    model_num += 1\n",
        "\n",
        "\n",
        "    # Log metrics and model\n",
        "    mlflow.spark.log_model(pipelineModel, modelName)\n",
        "    mlflow.log_metrics({\"modelNum\":model_num, \"accuracy\":accuracy, \"areaUnderROC\":roc, \"areaUnderPR\":pr})\n",
        "    print(\"Training complete:\",modelName)\n",
        "\n",
        "# Tree Based Models\n",
        "tree_models = [DecisionTreeClassifier(), RandomForestClassifier(), GBTClassifier()]\n",
        "for model in tree_models:\n",
        "    tree_pipeline = Pipeline(stages=[stringIndexer, oheEncoder, label_stringIdx, vecAssembler,model])\n",
        "    pipelineModel = tree_pipeline.fit(trainDF)\n",
        "    predDF = pipelineModel.transform(testDF)\n",
        "\n",
        "    modelName = str(model_num)+'-'+model.__class__.__name__\n",
        "    accuracy = mevaluator.setMetricName(\"accuracy\").evaluate(predDF)\n",
        "    roc = bevaluator.setMetricName(\"areaUnderROC\").evaluate(predDF)\n",
        "    pr = bevaluator.setMetricName(\"areaUnderPR\").evaluate(predDF)\n",
        "    model_num += 1\n",
        "\n",
        "    # Log metrics and model\n",
        "    mlflow.spark.log_model(pipelineModel, modelName)\n",
        "    mlflow.log_metrics({\"modelNum\":model_num, \"accuracy\":accuracy, \"areaUnderROC\":roc, \"areaUnderPR\":pr})\n",
        "    print(\"Training complete:\",modelName)\n",
        "\n",
        "\n",
        "pipelineModel.save('model')\n",
        "\n",
        "from azureml.core import Model\n",
        "Model.register(\n",
        "    workspace=ws,\n",
        "    model_path='model/',\n",
        "    model_name='pyspark-batch-leads-model',\n",
        ")\n",
        "\n",
        "mlflow.end_run()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete: 1-LogisticRegression\n",
            "Training complete: 2-LinearSVC\n",
            "Training complete: 3-DecisionTreeClassifier\n",
            "Training complete: 4-RandomForestClassifier\n",
            "Training complete: 5-GBTClassifier\n"
          ]
        }
      ],
      "execution_count": 56,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625984572731
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Compute"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "# Compute params\r\n",
        "compute_name = 'rohan-vm-cluster'\r\n",
        "inference_cluster = None\r\n",
        "\r\n",
        "if compute_name in ws.compute_targets:\r\n",
        "    inference_cluster = ComputeTarget(ws, compute_name)\r\n",
        "    print(\"Using existing cluster.\")\r\n",
        "else:\r\n",
        "    try:\r\n",
        "        compute_config = AmlCompute.provisioning_configuration(\r\n",
        "            vm_size ='STANDARD_DS11_V2', \r\n",
        "            max_nodes=2 )\r\n",
        "        inference_cluster = ComputeTarget.create(ws, compute_name, compute_config)\r\n",
        "        inference_cluster.wait_for_completion(show_output=True)\r\n",
        "    except Exception as ex:\r\n",
        "        print(ex)\r\n",
        "    print(\"Cluster created.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing cluster.\n"
          ]
        }
      ],
      "execution_count": 58,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625984618136
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scoring Script"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 'batch-pipeline/score.py'\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "from azureml.core import Model\r\n",
        "from pyspark.ml import PipelineModel\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "def init():\r\n",
        "    global model, columnsToBeRenamed, spark\r\n",
        "    spark = SparkSession.builder.getOrCreate()\r\n",
        "    model_path = Model.get_model_path('pyspark-batch-leads-model')\r\n",
        "    model= PipelineModel.load(model_path)\r\n",
        "    columnsToBeRenamed = {'emp.var.rate':'emp_var_rate','cons.price.idx':'cons_price_idx','cons.conf.idx':'cons_conf_idx','nr.employed':'nr_employed'}\r\n",
        "\r\n",
        "def run(mini_batch):\r\n",
        "    # This runs for each batch\r\n",
        "    resultList = []\r\n",
        "    # process each file in the batch\r\n",
        "    for f in mini_batch:\r\n",
        "        df = spark.read.csv(path=f,header=\"true\",inferSchema=\"true\",sep=\",\").drop('_c0')\r\n",
        "        for key in columnsToBeRenamed.keys():\r\n",
        "            df = df.withColumnRenamed(key, columnsToBeRenamed[key]) \r\n",
        "        prediction = model.transform(df).select('prediction').toPandas().prediction.map({0.0:\"no\",1.0:\"yes\"}).to_numpy()\r\n",
        "        resultList.append(\"{}: {}\".format(os.path.basename(f), prediction))\r\n",
        "    return resultList\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting batch-pipeline/score.py\n"
          ]
        }
      ],
      "execution_count": 63,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625970033791
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\r\n",
        "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\r\n",
        "\r\n",
        "# Create an Environment for the experiment\r\n",
        "batch_env = Environment.from_conda_specification(name=\"experiment_env\", file_path=\"batch-pipeline/batch_environment.yml\")\r\n",
        "batch_env.docker.base_image = DEFAULT_CPU_IMAGE\r\n",
        "print('Configuration ready.')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration ready.\n"
          ]
        }
      ],
      "execution_count": 64,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625985195670
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\r\n",
        "from azureml.data import OutputFileDatasetConfig\r\n",
        "from azureml.core.runconfig import DockerConfiguration\r\n",
        "\r\n",
        "output_dir = OutputFileDatasetConfig(name='inferences')\r\n",
        "\r\n",
        "parallel_run_config = ParallelRunConfig(\r\n",
        "    source_directory='batch-pipeline/',\r\n",
        "    entry_script=\"score.py\",\r\n",
        "    mini_batch_size=\"5\",\r\n",
        "    error_threshold=10,\r\n",
        "    output_action=\"append_row\",\r\n",
        "    environment=batch_env,\r\n",
        "    compute_target=inference_cluster,\r\n",
        "    node_count=2)\r\n",
        "\r\n",
        "parallelrun_step = ParallelRunStep(\r\n",
        "    name='batch-score-leads',\r\n",
        "    parallel_run_config=parallel_run_config,\r\n",
        "    inputs=[batch_data_set.as_named_input('leads_batch')],\r\n",
        "    output=output_dir,\r\n",
        "    arguments=[],\r\n",
        "    allow_reuse=True\r\n",
        ")\r\n",
        "\r\n",
        "print('Steps defined')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps defined\n"
          ]
        }
      ],
      "execution_count": 65,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625985197223
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\r\n",
        "from azureml.pipeline.core import Pipeline\r\n",
        "\r\n",
        "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\r\n",
        "pipeline_run = Experiment(workspace=ws, name='leads-batch-pipeline').submit(pipeline)\r\n",
        "pipeline_run.wait_for_completion(show_output=True)\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created step batch-score-leads [eee892ae][59ae9acc-052f-4555-8782-db60e9392eff], (This step will run and generate new outputs)\n",
            "Submitted PipelineRun 563a820c-275c-4174-9b9a-0d95de346706\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/563a820c-275c-4174-9b9a-0d95de346706?wsid=/subscriptions/23416925-66df-470c-b651-f378856d8ad7/resourcegroups/rohan-rg/workspaces/rohan-ws&tid=13715ad3-e049-4909-899b-f9e22f99b1a5\n",
            "PipelineRunId: 563a820c-275c-4174-9b9a-0d95de346706\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/563a820c-275c-4174-9b9a-0d95de346706?wsid=/subscriptions/23416925-66df-470c-b651-f378856d8ad7/resourcegroups/rohan-rg/workspaces/rohan-ws&tid=13715ad3-e049-4909-899b-f9e22f99b1a5\n",
            "PipelineRun Status: NotStarted\n",
            "PipelineRun Status: Running\n",
            "\n",
            "\n",
            "StepRunId: 120e54a8-7112-44dd-aca7-a81e179223c5\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/120e54a8-7112-44dd-aca7-a81e179223c5?wsid=/subscriptions/23416925-66df-470c-b651-f378856d8ad7/resourcegroups/rohan-rg/workspaces/rohan-ws&tid=13715ad3-e049-4909-899b-f9e22f99b1a5\n",
            "StepRun( batch-score-leads ) Status: NotStarted\n",
            "StepRun( batch-score-leads ) Status: Running\n",
            "\n",
            "Streaming azureml-logs/55_azureml-execution-tvmps_364464aacec038d412cda872ecf3ba5243b49d964d7fa7711f9608484bde8aed_d.txt\n",
            "========================================================================================================================\n",
            "2021-07-11T06:33:58Z Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/120e54a8-7112-44dd-aca7-a81e179223c5/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/120e54a8-7112-44dd-aca7-a81e179223c5/caches/workspaceblobstore -o ro --file-cache-timeout-in-seconds=1000000 --cache-size-mb=20151 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/120e54a8-7112-44dd-aca7-a81e179223c5/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
            "2021-07-11T06:33:58Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/120e54a8-7112-44dd-aca7-a81e179223c5/mounts/workspaceblobstore\n",
            "2021-07-11T06:33:58Z The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
            "2021-07-11T06:33:58Z Starting output-watcher...\n",
            "2021-07-11T06:33:58Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
            "Login Succeeded\n",
            "Using default tag: latest\n",
            "latest: Pulling from azureml/azureml_fe4afc798de401edfb76dc27a38b1703\n",
            "Digest: sha256:5224cd9c4e07c9304c90193ab084da3cf8643e81065eef4d81c2c4029c58248c\n",
            "Status: Image is up to date for viennaglobal.azurecr.io/azureml/azureml_fe4afc798de401edfb76dc27a38b1703:latest\n",
            "viennaglobal.azurecr.io/azureml/azureml_fe4afc798de401edfb76dc27a38b1703:latest\n",
            "2021-07-11T06:34:00Z The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
            "2021-07-11T06:34:00Z Check if container 120e54a8-7112-44dd-aca7-a81e179223c5_DataSidecar already exist exited with 0, \n",
            "\n",
            "cf41a862b200aa7e155564ee155941d9e7af5a0f58eb81f08e047e2141600c35\n",
            "2021-07-11T06:34:00Z Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
            "2021-07-11T06:34:00Z containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-b5fae98450d9a89e6893c14a7778114d-3e4a9edabb5b0e86-01 -sshRequired=false] \n",
            "2021/07/11 06:34:00 Starting App Insight Logger for task:  containerSetup\n",
            "2021/07/11 06:34:00 Version: 3.0.01632.0003 Branch: .SourceBranch Commit: 4b96fb0\n",
            "2021/07/11 06:34:00 Entered ContainerSetupTask - Preparing infiniband\n",
            "2021/07/11 06:34:00 Starting infiniband setup\n",
            "2021/07/11 06:34:00 Python Version found is Python 3.7.9\n",
            "\n",
            "2021/07/11 06:34:00 Returning Python Version as 3.7\n",
            "2021-07-11T06:34:00Z VMSize: standard_ds11_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
            "2021/07/11 06:34:00 VMSize: standard_ds11_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
            "2021/07/11 06:34:00 VMSize: standard_ds11_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
            "2021/07/11 06:34:00 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
            "2021/07/11 06:34:00 Not setting up Infiniband in Container\n",
            "2021/07/11 06:34:00 Not setting up Infiniband in Container\n",
            "2021-07-11T06:34:00Z Not setting up Infiniband in Container\n",
            "2021/07/11 06:34:00 Python Version found is Python 3.7.9\n",
            "\n",
            "2021/07/11 06:34:00 Returning Python Version as 3.7\n",
            "2021/07/11 06:34:00 sshd inside container not required for job, skipping setup.\n",
            "2021/07/11 06:34:00 All App Insights Logs was sent successfully or the close timeout of 20 was reached\n",
            "2021/07/11 06:34:00 App Insight Client has already been closed\n",
            "2021/07/11 06:34:00 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
            "Stopped: false\n",
            "OriginalData: 1\n",
            "FilteredData: 0.\n",
            "2021-07-11T06:34:00Z Starting docker container succeeded.\n",
            "\n",
            "Streaming azureml-logs/65_job_prep-tvmps_364464aacec038d412cda872ecf3ba5243b49d964d7fa7711f9608484bde8aed_d.txt\n",
            "===============================================================================================================\n",
            "[2021-07-11T06:34:01.846672] Entering job preparation.\n",
            "[2021-07-11T06:34:02.693748] Starting job preparation.\n",
            "[2021-07-11T06:34:02.693800] Extracting the control code.\n",
            "[2021-07-11T06:34:02.694349] Starting extract_project.\n",
            "[2021-07-11T06:34:02.694562] Starting to extract zip file.\n",
            "[2021-07-11T06:34:02.715756] Finished extracting zip file.\n",
            "[2021-07-11T06:34:02.718544] Using urllib.request Python 3.0 or later\n",
            "[2021-07-11T06:34:02.718747] Start fetching snapshots.\n",
            "[2021-07-11T06:34:02.719199] Start fetching snapshot.\n",
            "[2021-07-11T06:34:02.719391] Retrieving project from snapshot: 9cb194a5-971f-4d0f-ab8b-fe27a3e6e0e5\n",
            "Starting the daemon thread to refresh tokens in background for process with pid = 41\n",
            "[2021-07-11T06:34:03.001139] Finished fetching snapshot.\n",
            "[2021-07-11T06:34:03.001172] Start fetching snapshot.\n",
            "[2021-07-11T06:34:03.001196] Retrieving project from snapshot: d0c57f8d-9f67-42a2-8634-16a49782dd43\n",
            "[2021-07-11T06:34:11.595401] Finished fetching snapshot.\n",
            "[2021-07-11T06:34:11.595434] Finished fetching snapshots.\n",
            "[2021-07-11T06:34:11.595441] Finished extract_project.\n",
            "[2021-07-11T06:34:11.595647] Finished fetching and extracting the control code.\n",
            "[2021-07-11T06:34:11.603782] Start run_history_prep.\n",
            "[2021-07-11T06:34:11.610795] Job preparation is complete.\n",
            "[2021-07-11T06:34:11.610985] Entering Data Context Managers in Sidecar\n",
            "[2021-07-11T06:34:11.611798] Running Sidecar prep cmd...\n",
            "[2021-07-11T06:34:12.007849] INFO azureml.sidecar.sidecar: Received task: enter_contexts. Running on Linux at /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/120e54a8-7112-44dd-aca7-a81e179223c5/wd/azureml/120e54a8-7112-44dd-aca7-a81e179223c5\n",
            "[2021-07-11T06:34:12.008718] INFO azureml.sidecar.sidecar: Invoking \"enter_contexts\" task with Context Managers: {\"context_managers\": [\"Dataset:context_managers.Datasets\"]}\n",
            "Enter __enter__ of DatasetContextManager\n",
            "SDK version: azureml-core==1.28.0 azureml-dataprep==2.16.0. Session id: 8dc0ddf9-cebf-4f58-bac6-bdafa9a0bfff. Run id: 120e54a8-7112-44dd-aca7-a81e179223c5.\n",
            "Processing 'leads_batch'.\n",
            "Processing dataset FileDataset\n",
            "{\n",
            "  \"source\": [\n",
            "    \"('rohands', 'batch-data/')\"\n",
            "  ],\n",
            "  \"definition\": [\n",
            "    \"GetDatastoreFiles\"\n",
            "  ],\n",
            "  \"registration\": {\n",
            "    \"id\": \"c73804df-9317-415e-b2ed-32de72b2948b\",\n",
            "    \"name\": \"leads-batch-data\",\n",
            "    \"version\": 1,\n",
            "    \"description\": \"batch data for Marketing Leads UCI\",\n",
            "    \"workspace\": \"Workspace.create(name='rohan-ws', subscription_id='23416925-66df-470c-b651-f378856d8ad7', resource_group='rohan-rg')\"\n",
            "  }\n",
            "}\n",
            "Mounting leads_batch to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/120e54a8-7112-44dd-aca7-a81e179223c5/wd/leads_batch_c73804df-9317-415e-b2ed-32de72b2948b.\n",
            "Mounted leads_batch to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/120e54a8-7112-44dd-aca7-a81e179223c5/wd/leads_batch_c73804df-9317-415e-b2ed-32de72b2948b as folder.\n",
            "Processing 'inferences'.\n",
            "Mounted inferences to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/120e54a8-7112-44dd-aca7-a81e179223c5/wd/inferences_workspaceblobstore.\n",
            "Exit __enter__ of DatasetContextManager\n",
            "Set Dataset leads_batch's target path to /mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/120e54a8-7112-44dd-aca7-a81e179223c5/wd/leads_batch_c73804df-9317-415e-b2ed-32de72b2948b\n",
            "Set OutputDataset inferences's target path to /tmp/62184cfb-c612-46c2-b2b5-87aa1bd64f73\n",
            "[2021-07-11T06:34:28.603105] INFO azureml.sidecar.task.enter_contexts: Entered Context Managers\n",
            "[2021-07-11T06:34:29.484430] Ran Sidecar prep cmd.\n",
            "[2021-07-11T06:34:29.484613] Running Context Managers in Sidecar complete.\n",
            "\n",
            "Streaming azureml-logs/70_driver_log.txt\n",
            "========================================\n",
            "2021/07/11 06:35:35 Starting App Insight Logger for task:  runTaskLet\n",
            "2021/07/11 06:35:35 Version: 3.0.01632.0003 Branch: .SourceBranch Commit: 4b96fb0\n",
            "2021/07/11 06:35:35 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/info\n",
            "2021/07/11 06:35:35 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/status\n",
            "[2021-07-11T06:35:35.944577] Entering context manager injector.\n",
            "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'Dataset:context_managers.Datasets', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['driver/amlbi_main.py', '--client_sdk_version', '1.32.0', '--scoring_module_name', 'score.py', '--mini_batch_size', '5', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--output', 'DatasetOutputConfig:inferences', '--input_fds_0', 'leads_batch'])\n",
            "Script type = None\n",
            "[2021-07-11T06:35:36.542524] Entering Run History Context Manager.\n",
            "[2021-07-11T06:35:40.377394] Current directory: /mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/120e54a8-7112-44dd-aca7-a81e179223c5/wd/azureml/120e54a8-7112-44dd-aca7-a81e179223c5\n",
            "[2021-07-11T06:35:40.377434] Preparing to call script [driver/amlbi_main.py] with arguments:['--client_sdk_version', '1.32.0', '--scoring_module_name', 'score.py', '--mini_batch_size', '5', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--output', '$inferences', '--input_fds_0', 'leads_batch']\n",
            "[2021-07-11T06:35:40.377668] After variable expansion, calling script [driver/amlbi_main.py] with arguments:['--client_sdk_version', '1.32.0', '--scoring_module_name', 'score.py', '--mini_batch_size', '5', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--output', '/tmp/62184cfb-c612-46c2-b2b5-87aa1bd64f73', '--input_fds_0', 'leads_batch']\n",
            "\n",
            "2021/07/11 06:35:40 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
            "Stopped: false\n",
            "OriginalData: 1\n",
            "FilteredData: 0.\n",
            "\n",
            "Streaming azureml-logs/75_job_post-tvmps_364464aacec038d412cda872ecf3ba5243b49d964d7fa7711f9608484bde8aed_d.txt\n",
            "===============================================================================================================\n",
            "[2021-07-11T06:37:54.349433] Entering job release\n",
            "[2021-07-11T06:37:55.312715] job release stage : copy_batchai_cached_logs starting...\n",
            "[2021-07-11T06:37:55.312757] job release stage : copy_batchai_cached_logs completed...\n",
            "[2021-07-11T06:37:55.312974] Running in AzureML-Sidecar, starting to exit user context managers...\n",
            "[2021-07-11T06:37:55.313466] Running Sidecar release cmd...\n",
            "[2021-07-11T06:37:55.417184] INFO azureml.sidecar.sidecar: Received task: exit_contexts. Running on Linux at /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/120e54a8-7112-44dd-aca7-a81e179223c5/wd/azureml/120e54a8-7112-44dd-aca7-a81e179223c5\n",
            "Enter __exit__ of DatasetContextManager\n",
            "Unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/120e54a8-7112-44dd-aca7-a81e179223c5/wd/leads_batch_c73804df-9317-415e-b2ed-32de72b2948b.\n",
            "Finishing unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/120e54a8-7112-44dd-aca7-a81e179223c5/wd/leads_batch_c73804df-9317-415e-b2ed-32de72b2948b.\n",
            "Unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/120e54a8-7112-44dd-aca7-a81e179223c5/wd/inferences_workspaceblobstore.\n",
            "Finishing unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/120e54a8-7112-44dd-aca7-a81e179223c5/wd/inferences_workspaceblobstore.\n",
            "Exit __exit__ of DatasetContextManager\n",
            "[2021-07-11T06:37:55.467223] Removing absolute paths from host...\n",
            "[2021-07-11T06:37:55.467463] INFO azureml.sidecar.task.exit_contexts: Exited Context Managers\n",
            "[2021-07-11T06:37:55.930710] Ran Sidecar release cmd.\n",
            "\n",
            "StepRun(batch-score-leads) Execution Summary\n",
            "=============================================\n",
            "StepRun( batch-score-leads ) Status: Finished\n",
            "{'runId': '120e54a8-7112-44dd-aca7-a81e179223c5', 'target': 'rohan-vm-cluster', 'status': 'Completed', 'startTimeUtc': '2021-07-11T06:33:55.302231Z', 'endTimeUtc': '2021-07-11T06:38:08.852706Z', 'properties': {'ContentSnapshotId': '9cb194a5-971f-4d0f-ab8b-fe27a3e6e0e5', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': '59ae9acc-052f-4555-8782-db60e9392eff', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': 'eee892ae', 'azureml.pipelinerunid': '563a820c-275c-4174-9b9a-0d95de346706', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json', 'azureml.parallelrunstep': 'true'}, 'inputDatasets': [{'dataset': {'id': 'c73804df-9317-415e-b2ed-32de72b2948b'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'leads_batch', 'mechanism': 'Mount'}}, {'dataset': {'id': '97ff37a7-6efb-49fb-9837-889b463fdbff'}, 'consumptionDetails': {'type': 'Reference'}}], 'outputDatasets': [{'identifier': {'savedId': '97ff37a7-6efb-49fb-9837-889b463fdbff'}, 'outputType': 'RunOutput', 'outputDetails': {'outputName': 'inferences'}, 'dataset': {\n",
            "  \"source\": [\n",
            "    \"('workspaceblobstore', 'dataset/120e54a8-7112-44dd-aca7-a81e179223c5/inferences/')\"\n",
            "  ],\n",
            "  \"definition\": [\n",
            "    \"GetDatastoreFiles\"\n",
            "  ],\n",
            "  \"registration\": {\n",
            "    \"id\": \"97ff37a7-6efb-49fb-9837-889b463fdbff\",\n",
            "    \"name\": null,\n",
            "    \"version\": null,\n",
            "    \"workspace\": \"Workspace.create(name='rohan-ws', subscription_id='23416925-66df-470c-b651-f378856d8ad7', resource_group='rohan-rg')\"\n",
            "  }\n",
            "}}], 'runDefinition': {'script': 'driver/amlbi_main.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--client_sdk_version', '1.32.0', '--scoring_module_name', 'score.py', '--mini_batch_size', '5', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--output', 'DatasetOutputConfig:inferences', '--input_fds_0', 'leads_batch'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'rohan-vm-cluster', 'dataReferences': {}, 'data': {'leads_batch': {'dataLocation': {'dataset': {'id': 'c73804df-9317-415e-b2ed-32de72b2948b', 'name': None, 'version': '1'}, 'dataPath': None, 'uri': None}, 'mechanism': 'Mount', 'environmentVariableName': 'leads_batch', 'pathOnCompute': None, 'overwrite': False}}, 'outputData': {'inferences': {'outputLocation': {'dataset': None, 'dataPath': {'datastoreName': 'workspaceblobstore', 'relativePath': None}, 'uri': None}, 'mechanism': 'Mount', 'additionalOptions': {'pathOnCompute': '/tmp/62184cfb-c612-46c2-b2b5-87aa1bd64f73/', 'registrationOptions': {'name': None, 'description': None, 'tags': None, 'datasetRegistrationOptions': {'additionalTransformation': None}}, 'uploadOptions': {'overwrite': False, 'sourceGlobs': {'globPatterns': None}}, 'mountOptions': None}}}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 2, 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'experiment_env', 'version': 'Autosave_2021-07-11T05:41:00Z_d8e992ec', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['defaults', 'anaconda', 'conda-forge'], 'dependencies': ['python=3.6', 'scikit-learn', 'pip', 'pandas', 'numpy', 'openjdk', {'pip': ['pyspark', 'mlflow', 'azureml-mlflow', 'azureml-defaults']}], 'name': 'azureml_f2739031ae37a487a0872ec6d8eff353'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210615.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': None, 'imageVersion': None, 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_364464aacec038d412cda872ecf3ba5243b49d964d7fa7711f9608484bde8aed_d.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/azureml-logs/55_azureml-execution-tvmps_364464aacec038d412cda872ecf3ba5243b49d964d7fa7711f9608484bde8aed_d.txt?sv=2019-02-02&sr=b&sig=sXvVU05FjcDa%2FJYpgiWZSQGHli8epzL00iB0jrspVjM%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'azureml-logs/55_azureml-execution-tvmps_377944c8263cd5009351ddaa7b201ea2219fe0599e1209fdcc0abe571eebf272_d.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/azureml-logs/55_azureml-execution-tvmps_377944c8263cd5009351ddaa7b201ea2219fe0599e1209fdcc0abe571eebf272_d.txt?sv=2019-02-02&sr=b&sig=StL06uX4SWbSPsR3JdsNCZH7yRz76LOSmVmalgoiWqE%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'azureml-logs/65_job_prep-tvmps_364464aacec038d412cda872ecf3ba5243b49d964d7fa7711f9608484bde8aed_d.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/azureml-logs/65_job_prep-tvmps_364464aacec038d412cda872ecf3ba5243b49d964d7fa7711f9608484bde8aed_d.txt?sv=2019-02-02&sr=b&sig=SrqbpxSoFdLOQ08c5kfFbKgyT40ZMtxR0FufbXEi%2B%2FA%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'azureml-logs/65_job_prep-tvmps_377944c8263cd5009351ddaa7b201ea2219fe0599e1209fdcc0abe571eebf272_d.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/azureml-logs/65_job_prep-tvmps_377944c8263cd5009351ddaa7b201ea2219fe0599e1209fdcc0abe571eebf272_d.txt?sv=2019-02-02&sr=b&sig=S70G5%2Bd1HlbrNStl21xZ5d3i1MHiotYlm04R0uECuYc%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=DqwzpPiKtUZVzSissReTTFGv26%2BJ50hLJgUMmRuPLgg%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'azureml-logs/75_job_post-tvmps_364464aacec038d412cda872ecf3ba5243b49d964d7fa7711f9608484bde8aed_d.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/azureml-logs/75_job_post-tvmps_364464aacec038d412cda872ecf3ba5243b49d964d7fa7711f9608484bde8aed_d.txt?sv=2019-02-02&sr=b&sig=or9UWzjjaLw3pyA%2BM5Ex6lXYw1isbw7r8XZAR6XvMU8%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'azureml-logs/75_job_post-tvmps_377944c8263cd5009351ddaa7b201ea2219fe0599e1209fdcc0abe571eebf272_d.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/azureml-logs/75_job_post-tvmps_377944c8263cd5009351ddaa7b201ea2219fe0599e1209fdcc0abe571eebf272_d.txt?sv=2019-02-02&sr=b&sig=NSuCAl3P%2FzgQp8KsjNRRRvcxSkkGkNofh2b4bIBFRnw%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'azureml-logs/process_info.json': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=4nzjgFsVHV1Pwjb682sXRPERiq0juecsgIPn12YW1Xg%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'azureml-logs/process_status.json': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=4MNVlHdAWpAKKiIn6c1Ad5VIDSf8Koik6U1qxYrnRlU%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'logs/azureml/104_azureml.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/logs/azureml/104_azureml.log?sv=2019-02-02&sr=b&sig=qsm%2Bcnr38dg5%2FIhv1SB0KGd25oFVbwu9NVKtu6Bs3FY%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'logs/azureml/87_azureml.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/logs/azureml/87_azureml.log?sv=2019-02-02&sr=b&sig=MQVvUS%2Fj3PNwragJjjEOTKiTrPSTS2U4MUGK1aA3E3Q%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'logs/azureml/dataprep/backgroundProcess.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/logs/azureml/dataprep/backgroundProcess.log?sv=2019-02-02&sr=b&sig=PG1wXGj67F%2BuxnQw0e5IusdVEOF5hYIRoYAlz7pJ5oI%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-02-02&sr=b&sig=APS81fWGTYeOEKBbbnYwKPxQU49hNio6TiSWhEtgtA4%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=lXYtOFiRbgnHhILTk2mJmTAETJ9zRS9SuHF77vQFjko%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'logs/azureml/job_prep_azureml.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=rb%2FLFz%2BYskEEs9rLMNlc8SuBXCcOvFYqklnniI3npoc%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'logs/azureml/job_release_azureml.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=iL%2BWRl%2FoBPx2IFySlpDNfCxY%2FIcFkHXnR7tUFmjAX8g%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'logs/azureml/sidecar/tvmps_364464aacec038d412cda872ecf3ba5243b49d964d7fa7711f9608484bde8aed_d/all.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/logs/azureml/sidecar/tvmps_364464aacec038d412cda872ecf3ba5243b49d964d7fa7711f9608484bde8aed_d/all.log?sv=2019-02-02&sr=b&sig=tFL8Nzg62njMce%2BtR10WQixp0Y7Vjrp0zogSTwILqhU%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'logs/azureml/sidecar/tvmps_364464aacec038d412cda872ecf3ba5243b49d964d7fa7711f9608484bde8aed_d/task.enter_contexts.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/logs/azureml/sidecar/tvmps_364464aacec038d412cda872ecf3ba5243b49d964d7fa7711f9608484bde8aed_d/task.enter_contexts.log?sv=2019-02-02&sr=b&sig=hjmmZ74ag0gmqo0cBc27UDXWjfWH2UUMT8MSSPT2VbU%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'logs/azureml/sidecar/tvmps_364464aacec038d412cda872ecf3ba5243b49d964d7fa7711f9608484bde8aed_d/task.exit_contexts.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/logs/azureml/sidecar/tvmps_364464aacec038d412cda872ecf3ba5243b49d964d7fa7711f9608484bde8aed_d/task.exit_contexts.log?sv=2019-02-02&sr=b&sig=oGu99%2FPBpIZjWgXKNhICsbaPZq2Gy%2Fls7HaF0eBz9%2FI%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'logs/azureml/sidecar/tvmps_377944c8263cd5009351ddaa7b201ea2219fe0599e1209fdcc0abe571eebf272_d/all.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/logs/azureml/sidecar/tvmps_377944c8263cd5009351ddaa7b201ea2219fe0599e1209fdcc0abe571eebf272_d/all.log?sv=2019-02-02&sr=b&sig=ZEgcCQ0TPXydyTocEzipeoqhdPcEvfvMDc5OqQ26YoY%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'logs/azureml/sidecar/tvmps_377944c8263cd5009351ddaa7b201ea2219fe0599e1209fdcc0abe571eebf272_d/task.enter_contexts.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/logs/azureml/sidecar/tvmps_377944c8263cd5009351ddaa7b201ea2219fe0599e1209fdcc0abe571eebf272_d/task.enter_contexts.log?sv=2019-02-02&sr=b&sig=WUUNnLE%2F1t3M76n04riuYy29SlFrSrKJUCjqtTP9%2FdM%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'logs/azureml/sidecar/tvmps_377944c8263cd5009351ddaa7b201ea2219fe0599e1209fdcc0abe571eebf272_d/task.exit_contexts.log': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/logs/azureml/sidecar/tvmps_377944c8263cd5009351ddaa7b201ea2219fe0599e1209fdcc0abe571eebf272_d/task.exit_contexts.log?sv=2019-02-02&sr=b&sig=9zAPwjQe1aw61hiOyUrxPp0Yv9fBwRhMSypMWcpWM%2Fc%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=eSx27GDeakUGgyPD8oWcFDh7mgsDfSsC5CF30WxwJdc%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.120e54a8-7112-44dd-aca7-a81e179223c5/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=WSv5IYJMCPZ1NTYOODKerBqUGrr3s7MRJgdczDBcl78%3D&st=2021-07-11T06%3A27%3A58Z&se=2021-07-11T14%3A37%3A58Z&sp=r'}, 'submittedBy': 'Aishwarya Singh'}\n",
            "\n",
            "\n",
            "\n",
            "PipelineRun Execution Summary\n",
            "==============================\n",
            "PipelineRun Status: Finished\n",
            "{'runId': '563a820c-275c-4174-9b9a-0d95de346706', 'status': 'Completed', 'startTimeUtc': '2021-07-11T06:33:30.533986Z', 'endTimeUtc': '2021-07-11T06:38:13.241635Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.563a820c-275c-4174-9b9a-0d95de346706/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=Q9T6Rgl6CzNJMZ2sLmZwdncFEeOJAWAgvdvv9KX5bZ0%3D&st=2021-07-11T06%3A24%3A18Z&se=2021-07-11T14%3A34%3A18Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.563a820c-275c-4174-9b9a-0d95de346706/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=XAmKBpl0LvcbsCiLS8voqwWj1vIfT0F4MMt8hXDPiEk%3D&st=2021-07-11T06%3A24%3A18Z&se=2021-07-11T14%3A34%3A18Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.563a820c-275c-4174-9b9a-0d95de346706/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=OAnyI%2BYw1NBVYK7Ksaf9cr9yR2PC1K0mXCat%2FRIMKv0%3D&st=2021-07-11T06%3A24%3A18Z&se=2021-07-11T14%3A34%3A18Z&sp=r'}, 'submittedBy': 'Aishwarya Singh'}\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 66,
          "data": {
            "text/plain": "'Finished'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 66,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625985493574
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "# Remove the local results folder if left over from a previous run\n",
        "result_file = None\n",
        "shutil.rmtree('leads-results', ignore_errors=True)\n",
        "\n",
        "# Get the run for the first step and download its output\n",
        "prediction_run = next(pipeline_run.get_children())\n",
        "prediction_output = prediction_run.get_output_data('inferences')\n",
        "prediction_output.download(local_path='leads-results')\n",
        "\n",
        "# Traverse the folder hierarchy and find the results file\n",
        "for root, dirs, files in os.walk('leads-results'):\n",
        "    for file in files:\n",
        "        if file.endswith('parallel_run_step.txt'):\n",
        "            result_file = os.path.join(root,file)\n",
        "\n",
        "# cleanup output format\n",
        "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
        "df.columns = [\"File\", \"Prediction\"]\n",
        "df.to_csv(\"leads-results/leads.csv\", header=True,index=False)\n",
        "# Display the first 20 results\n",
        "df.head(20)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 76,
          "data": {
            "text/plain": "      File                                         Prediction\n0    1.csv   ['no' 'no' 'no' 'no' 'no' 'yes' 'no' 'no' 'no...\n1   10.csv   ['no' 'yes' 'no' 'no' 'no' 'no' 'no' 'no' 'no...\n2   11.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...\n3   12.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...\n4   13.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...\n5   14.csv   ['no' 'yes' 'no' 'no' 'no' 'no' 'no' 'no' 'no...\n6   15.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...\n7   16.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...\n8   17.csv   ['yes' 'no' 'no' 'yes' 'no' 'no' 'no' 'no' 'n...\n9   18.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...\n10  19.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'yes' 'no' 'no...\n11   2.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'yes...\n12  20.csv   ['yes' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no...\n13  21.csv   ['no' 'no' 'no' 'yes' 'no' 'no' 'no' 'no' 'no...\n14  22.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...\n15  23.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...\n16  24.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...\n17  25.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'yes...\n18  26.csv   ['no' 'no' 'no' 'yes' 'no' 'no' 'no' 'no' 'no...\n19  27.csv   ['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>File</th>\n      <th>Prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'yes' 'no' 'no' 'no...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10.csv</td>\n      <td>['no' 'yes' 'no' 'no' 'no' 'no' 'no' 'no' 'no...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>12.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>14.csv</td>\n      <td>['no' 'yes' 'no' 'no' 'no' 'no' 'no' 'no' 'no...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>15.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>16.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>17.csv</td>\n      <td>['yes' 'no' 'no' 'yes' 'no' 'no' 'no' 'no' 'n...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>18.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>19.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'yes' 'no' 'no...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'yes...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>20.csv</td>\n      <td>['yes' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>21.csv</td>\n      <td>['no' 'no' 'no' 'yes' 'no' 'no' 'no' 'no' 'no...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>22.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>23.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>24.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>25.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'yes...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>26.csv</td>\n      <td>['no' 'no' 'no' 'yes' 'no' 'no' 'no' 'no' 'no...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>27.csv</td>\n      <td>['no' 'no' 'no' 'no' 'no' 'no' 'no' 'no' 'no'...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 76,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625996182645
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "published_pipeline = pipeline_run.publish_pipeline(name='leads-batch-pipeline', description='Batch scoring of leads data from UCI', version='1.0')\r\n",
        "rest_endpoint = published_pipeline.endpoint\r\n",
        "print(rest_endpoint)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://centralindia.api.azureml.ms/pipelines/v1.0/subscriptions/23416925-66df-470c-b651-f378856d8ad7/resourceGroups/rohan-rg/providers/Microsoft.MachineLearningServices/workspaces/rohan-ws/PipelineRuns/PipelineSubmit/32f710e7-5a0e-48cb-b15c-f83d9ca9b906\n"
          ]
        }
      ],
      "execution_count": 77,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625996243014
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the Pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.authentication import InteractiveLoginAuthentication\r\n",
        "from azureml.pipeline.core.run import PipelineRun\r\n",
        "from azureml.widgets import RunDetails\r\n",
        "import requests\r\n",
        "\r\n",
        "interactive_auth = InteractiveLoginAuthentication()\r\n",
        "auth_header = interactive_auth.get_authentication_header()\r\n",
        "\r\n",
        "rest_endpoint = published_pipeline.endpoint\r\n",
        "response = requests.post(rest_endpoint, headers=auth_header, json={\"ExperimentName\": \"leads-batch-pipeline\"})\r\n",
        "run_id = response.json()[\"Id\"]\r\n",
        "\r\n",
        "published_pipeline_run = PipelineRun(ws.experiments['leads-batch-pipeline'], run_id)\r\n",
        "published_pipeline_run.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PipelineRunId: 468ad7bc-f419-411d-8abe-f1411b8843a4\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/468ad7bc-f419-411d-8abe-f1411b8843a4?wsid=/subscriptions/23416925-66df-470c-b651-f378856d8ad7/resourcegroups/rohan-rg/workspaces/rohan-ws&tid=13715ad3-e049-4909-899b-f9e22f99b1a5\n",
            "PipelineRun Status: NotStarted\n",
            "PipelineRun Status: Running\n",
            "\n",
            "PipelineRun Execution Summary\n",
            "==============================\n",
            "PipelineRun Status: Finished\n",
            "{'runId': '468ad7bc-f419-411d-8abe-f1411b8843a4', 'status': 'Completed', 'startTimeUtc': '2021-07-11T09:37:32.936711Z', 'endTimeUtc': '2021-07-11T09:37:36.01278Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'Unavailable', 'runType': 'HTTP', 'azureml.parameters': '{}', 'azureml.pipelineid': '32f710e7-5a0e-48cb-b15c-f83d9ca9b906'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.468ad7bc-f419-411d-8abe-f1411b8843a4/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=Ti3FZhwAnOXT2axAkIJUSHNNxxLqGptao4AG5nxY1Wc%3D&st=2021-07-11T09%3A27%3A37Z&se=2021-07-11T17%3A37%3A37Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.468ad7bc-f419-411d-8abe-f1411b8843a4/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=tghC%2BbSuWbs%2F7bk4twdBUJTtlXJymPb70im5ssf8kJ4%3D&st=2021-07-11T09%3A27%3A37Z&se=2021-07-11T17%3A37%3A37Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://rohansa.blob.core.windows.net/azureml/ExperimentRun/dcid.468ad7bc-f419-411d-8abe-f1411b8843a4/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=tXzw9SxCEbaV5%2Bwxqad8pV1nDeWs46RyUpQnsl%2BDiHE%3D&st=2021-07-11T09%3A27%3A37Z&se=2021-07-11T17%3A37%3A37Z&sp=r'}, 'submittedBy': 'Aishwarya Singh'}\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 78,
          "data": {
            "text/plain": "'Finished'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 78,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625996257129
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}