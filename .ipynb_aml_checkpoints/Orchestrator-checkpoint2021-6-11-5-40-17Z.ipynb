{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Configure Workspace"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "from azureml.core import Workspace, Experiment\r\n",
        "import mlflow\r\n",
        "\r\n",
        "# Setup Azure Workspace\r\n",
        "ws = Workspace.from_config()\r\n",
        "experiment_name = 'leads-pyspark-train'\r\n",
        "experiment = Experiment(workspace=ws, name=experiment_name)\r\n",
        "\r\n",
        "# Start MLflow Experiment\r\n",
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\r\n",
        "mlflow.set_experiment(experiment_name)\r\n",
        "run = mlflow.start_run()\r\n",
        "\r\n",
        "# Get default datastore\r\n",
        "default_ds = ws.get_default_datastore()\r\n",
        "\r\n",
        "# Get Spark session\r\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625974117242
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate and Upload Batch Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "\r\n",
        "df = spark.read.csv(\r\n",
        "    path='data/bank-additional-full.csv',\r\n",
        "    header=\"true\",\r\n",
        "    inferSchema=\"true\",\r\n",
        "    sep=\";\")\r\n",
        "trainDF, testDF, batchDF = df.randomSplit([.7, .29, .01], seed=999)\r\n",
        "batchData = batchDF.toPandas()\r\n",
        "\r\n",
        "# Create a folder\r\n",
        "batch_folder = './batch-data'\r\n",
        "os.makedirs(batch_folder, exist_ok=True)\r\n",
        "print(\"Folder created!\")\r\n",
        "\r\n",
        "# Save each sample as a separate file\r\n",
        "print(\"Saving files...\")\r\n",
        "x = 0\r\n",
        "y = 10\r\n",
        "for i in range(int(batchDF.count()/10)):\r\n",
        "    filename = str(i+1) + '.csv'\r\n",
        "    writeData=batchData[x:y]\r\n",
        "    writeData.to_csv(os.path.join(batch_folder, filename), sep=\",\")\r\n",
        "    x+=10\r\n",
        "    y+=10\r\n",
        "\r\n",
        "print(\"files saved!\")\r\n",
        "\r\n",
        "# Upload the files to the default datastore\r\n",
        "print(\"Uploading files to datastore...\")\r\n",
        "default_ds = ws.get_default_datastore()\r\n",
        "default_ds.upload(src_dir=\"batch-data\", target_path=\"batch-data\", overwrite=True, show_progress=True)\r\n",
        "\r\n",
        "# Register a dataset for the input data\r\n",
        "batch_data_set = Dataset.File.from_files(path=(default_ds, 'batch-data/'), validate=False)\r\n",
        "try:\r\n",
        "    batch_data_set = batch_data_set.register(workspace=ws, \r\n",
        "                                             name='leads-batch-data',\r\n",
        "                                             description='batch data for Marketing Leads UCI',\r\n",
        "                                             create_new_version=True)\r\n",
        "except Exception as ex:\r\n",
        "    print(ex)\r\n",
        "\r\n",
        "print(\"Done!\")\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder created!\n",
            "Saving files...\n",
            "files saved!\n",
            "Uploading files to datastore...\n",
            "Uploading an estimated of 40 files\n",
            "Uploading batch-data/1.csv\n",
            "Uploaded batch-data/1.csv, 1 files out of an estimated total of 40\n",
            "Uploading batch-data/10.csv\n",
            "Uploaded batch-data/10.csv, 2 files out of an estimated total of 40\n",
            "Uploading batch-data/11.csv\n",
            "Uploaded batch-data/11.csv, 3 files out of an estimated total of 40\n",
            "Uploading batch-data/12.csv\n",
            "Uploaded batch-data/12.csv, 4 files out of an estimated total of 40\n",
            "Uploading batch-data/13.csv\n",
            "Uploaded batch-data/13.csv, 5 files out of an estimated total of 40\n",
            "Uploading batch-data/14.csv\n",
            "Uploaded batch-data/14.csv, 6 files out of an estimated total of 40\n",
            "Uploading batch-data/15.csv\n",
            "Uploaded batch-data/15.csv, 7 files out of an estimated total of 40\n",
            "Uploading batch-data/16.csv\n",
            "Uploaded batch-data/16.csv, 8 files out of an estimated total of 40\n",
            "Uploading batch-data/17.csv\n",
            "Uploaded batch-data/17.csv, 9 files out of an estimated total of 40\n",
            "Uploading batch-data/18.csv\n",
            "Uploaded batch-data/18.csv, 10 files out of an estimated total of 40\n",
            "Uploading batch-data/19.csv\n",
            "Uploaded batch-data/19.csv, 11 files out of an estimated total of 40\n",
            "Uploading batch-data/2.csv\n",
            "Uploaded batch-data/2.csv, 12 files out of an estimated total of 40\n",
            "Uploading batch-data/20.csv\n",
            "Uploaded batch-data/20.csv, 13 files out of an estimated total of 40\n",
            "Uploading batch-data/21.csv\n",
            "Uploaded batch-data/21.csv, 14 files out of an estimated total of 40\n",
            "Uploading batch-data/22.csv\n",
            "Uploaded batch-data/22.csv, 15 files out of an estimated total of 40\n",
            "Uploading batch-data/23.csv\n",
            "Uploaded batch-data/23.csv, 16 files out of an estimated total of 40\n",
            "Uploading batch-data/24.csv\n",
            "Uploaded batch-data/24.csv, 17 files out of an estimated total of 40\n",
            "Uploading batch-data/25.csv\n",
            "Uploaded batch-data/25.csv, 18 files out of an estimated total of 40\n",
            "Uploading batch-data/26.csv\n",
            "Uploaded batch-data/26.csv, 19 files out of an estimated total of 40\n",
            "Uploading batch-data/27.csv\n",
            "Uploaded batch-data/27.csv, 20 files out of an estimated total of 40\n",
            "Uploading batch-data/28.csv\n",
            "Uploaded batch-data/28.csv, 21 files out of an estimated total of 40\n",
            "Uploading batch-data/29.csv\n",
            "Uploaded batch-data/29.csv, 22 files out of an estimated total of 40\n",
            "Uploading batch-data/3.csv\n",
            "Uploaded batch-data/3.csv, 23 files out of an estimated total of 40\n",
            "Uploading batch-data/30.csv\n",
            "Uploaded batch-data/30.csv, 24 files out of an estimated total of 40\n",
            "Uploading batch-data/31.csv\n",
            "Uploaded batch-data/31.csv, 25 files out of an estimated total of 40\n",
            "Uploading batch-data/32.csv\n",
            "Uploaded batch-data/32.csv, 26 files out of an estimated total of 40\n",
            "Uploading batch-data/33.csv\n",
            "Uploaded batch-data/33.csv, 27 files out of an estimated total of 40\n",
            "Uploading batch-data/34.csv\n",
            "Uploaded batch-data/34.csv, 28 files out of an estimated total of 40\n",
            "Uploading batch-data/35.csv\n",
            "Uploaded batch-data/35.csv, 29 files out of an estimated total of 40\n",
            "Uploading batch-data/36.csv\n",
            "Uploaded batch-data/36.csv, 30 files out of an estimated total of 40\n",
            "Uploading batch-data/37.csv\n",
            "Uploaded batch-data/37.csv, 31 files out of an estimated total of 40\n",
            "Uploading batch-data/38.csv\n",
            "Uploaded batch-data/38.csv, 32 files out of an estimated total of 40\n",
            "Uploading batch-data/39.csv\n",
            "Uploaded batch-data/39.csv, 33 files out of an estimated total of 40\n",
            "Uploading batch-data/4.csv\n",
            "Uploaded batch-data/4.csv, 34 files out of an estimated total of 40\n",
            "Uploading batch-data/40.csv\n",
            "Uploaded batch-data/40.csv, 35 files out of an estimated total of 40\n",
            "Uploading batch-data/5.csv\n",
            "Uploaded batch-data/5.csv, 36 files out of an estimated total of 40\n",
            "Uploading batch-data/6.csv\n",
            "Uploaded batch-data/6.csv, 37 files out of an estimated total of 40\n",
            "Uploading batch-data/7.csv\n",
            "Uploaded batch-data/7.csv, 38 files out of an estimated total of 40\n",
            "Uploading batch-data/8.csv\n",
            "Uploaded batch-data/8.csv, 39 files out of an estimated total of 40\n",
            "Uploading batch-data/9.csv\n",
            "Uploaded batch-data/9.csv, 40 files out of an estimated total of 40\n",
            "Uploaded 40 files\n",
            "Done!\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625974133088
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A Transformer used in pipelines for renaming columns \n",
        "from pyspark.ml import Transformer\n",
        "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable  \n",
        "\n",
        "class ColumnRenamer(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
        "    '''\n",
        "    Renames the following columns in the dataframe: \n",
        "    employment variation rate\n",
        "    consumer price index\n",
        "    consumer confidence index \n",
        "    number of employees \n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(ColumnRenamer, self).__init__()\n",
        "        self.columnsToBeRenamed = {\n",
        "            'emp.var.rate':'emp_var_rate',\n",
        "            'cons.price.idx':'cons_price_idx',\n",
        "            'cons.conf.idx':'cons_conf_idx',\n",
        "            'nr.employed':'nr_employed'}\n",
        "\n",
        "    def _transform(self, df):\n",
        "        for key in self.columnsToBeRenamed.keys():\n",
        "            df = df.withColumnRenamed(key, self.columnsToBeRenamed[key])\n",
        "        return df    \n",
        "rename_columns = ColumnRenamer()\n",
        "\n",
        "# Uses R Formula for automatic conversion of categorical labels to 1 hot encoding\n",
        "from pyspark.ml.feature import RFormula\n",
        "rFormula = RFormula(formula=\"y ~ .\", featuresCol=\"features\", labelCol=\"label\", handleInvalid=\"skip\")\n",
        "\n",
        "# Uses String Indexer and Numeric Columns only for Tree Based Classifiers\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "example_df = rename_columns.transform(trainDF)\n",
        "categorialColumns = [colname for (colname, dataType) in example_df.dtypes if ((dataType==\"string\") and (colname!=\"y\"))]\n",
        "stringIndexer = StringIndexer(inputCols=categorialColumns, outputCols=[c + \"Index\" for c in categorialColumns])\n",
        "oheEncoder = OneHotEncoder(inputCols=stringIndexer.getOutputCols(), outputCols=[c + \"ohe\" for c in categorialColumns])\n",
        "label_stringIdx = StringIndexer(inputCol=\"y\", outputCol=\"label\")\n",
        "numericColumns = [colname for (colname, dataType) in example_df.dtypes if (dataType==\"int\" or dataType==\"float\" or dataType==\"double\")]\n",
        "assembledInputs = numericColumns + [c + \"Index\" for c in categorialColumns]\n",
        "vecAssembler = VectorAssembler(inputCols=assembledInputs, outputCol=\"features\")"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625974133461
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, LinearSVC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "import mlflow.spark\n",
        "import pandas as pd\n",
        "\n",
        "# For Tracking Models\n",
        "model_num=1\n",
        "pipelineModel = None\n",
        "\n",
        "# Evaluators for performance metrics\n",
        "bevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\")\n",
        "mevaluator = MulticlassClassificationEvaluator()\n",
        "\n",
        "# Non Tree Based Models\n",
        "non_tree_models = [LogisticRegression(), LinearSVC()]\n",
        "for model in non_tree_models:\n",
        "    non_tree_pipeline = Pipeline(stages=[rename_columns, rFormula, model])\n",
        "    pipelineModel = non_tree_pipeline.fit(trainDF)\n",
        "    predDF = pipelineModel.transform(testDF)\n",
        "\n",
        "    modelName =str(model_num)+'-'+model.__class__.__name__\n",
        "    accuracy = mevaluator.setMetricName(\"accuracy\").evaluate(predDF)\n",
        "    roc = bevaluator.setMetricName(\"areaUnderROC\").evaluate(predDF)\n",
        "    pr = bevaluator.setMetricName(\"areaUnderPR\").evaluate(predDF)\n",
        "    model_num += 1\n",
        "\n",
        "\n",
        "    # Log metrics and model\n",
        "    mlflow.spark.log_model(pipelineModel, modelName)\n",
        "    mlflow.log_metrics({\"modelNum\":model_num, \"accuracy\":accuracy, \"areaUnderROC\":roc, \"areaUnderPR\":pr})\n",
        "    print(\"Training complete:\",modelName)\n",
        "\n",
        "# Tree Based Models\n",
        "tree_models = [DecisionTreeClassifier(), RandomForestClassifier(), GBTClassifier()]\n",
        "for model in tree_models:\n",
        "    tree_pipeline = Pipeline(stages=[rename_columns, stringIndexer, oheEncoder, label_stringIdx, vecAssembler,model])\n",
        "    pipelineModel = tree_pipeline.fit(trainDF)\n",
        "    predDF = pipelineModel.transform(testDF)\n",
        "\n",
        "    modelName = str(model_num)+'-'+model.__class__.__name__\n",
        "    accuracy = mevaluator.setMetricName(\"accuracy\").evaluate(predDF)\n",
        "    roc = bevaluator.setMetricName(\"areaUnderROC\").evaluate(predDF)\n",
        "    pr = bevaluator.setMetricName(\"areaUnderPR\").evaluate(predDF)\n",
        "    model_num += 1\n",
        "\n",
        "    # Log metrics and model\n",
        "    mlflow.spark.log_model(pipelineModel, modelName)\n",
        "    mlflow.log_metrics({\"modelNum\":model_num, \"accuracy\":accuracy, \"areaUnderROC\":roc, \"areaUnderPR\":pr})\n",
        "    print(\"Training complete:\",modelName)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete: 1-LogisticRegression\n",
            "Training complete: 2-LinearSVC\n",
            "Training complete: 3-DecisionTreeClassifier\n",
            "Training complete: 4-RandomForestClassifier\n",
            "Training complete: 5-GBTClassifier\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625974340045
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "gbt = GBTClassifier()\n",
        "gbt_pipeline = Pipeline(stages=[rename_columns, stringIndexer, oheEncoder, label_stringIdx, vecAssembler, gbt])\n",
        "paramGrid = ParamGridBuilder().addGrid(gbt.maxDepth,[5,10]).build()\n",
        "    \n",
        "cv = CrossValidator(estimator=gbt_pipeline, estimatorParamMaps=paramGrid, evaluator=mevaluator, numFolds=5)\n",
        "cvModel = cv.fit(trainDF)\n",
        "predictions = cvModel.transform(testDF)\n",
        "bevaluator.evaluate(predictions)\n",
        "'''\n",
        "\n",
        "pipelineModel.save('model')\n",
        "\n",
        "from azureml.core import Model\n",
        "Model.register(\n",
        "    workspace=ws,\n",
        "    model_path='model/',\n",
        "    model_name='pyspark-batch-leads-model',\n",
        ")\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registering model pyspark-batch-leads-model\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625974362348
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Compute"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "# Compute params\r\n",
        "compute_name = 'rohan-vm-cluster'\r\n",
        "inference_cluster = None\r\n",
        "\r\n",
        "if compute_name in ws.compute_targets:\r\n",
        "    inference_cluster = ComputeTarget(ws, compute_name)\r\n",
        "    print(\"Using existing cluster.\")\r\n",
        "else:\r\n",
        "    try:\r\n",
        "        compute_config = AmlCompute.provisioning_configuration(\r\n",
        "            vm_size ='STANDARD_DS11_V2', \r\n",
        "            max_nodes=2 )\r\n",
        "        inference_cluster = ComputeTarget.create(ws, compute_name, compute_config)\r\n",
        "        inference_cluster.wait_for_completion(show_output=True)\r\n",
        "    except Exception as ex:\r\n",
        "        print(ex)\r\n",
        "    print(\"Cluster created.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing cluster.\n"
          ]
        }
      ],
      "execution_count": 21,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625975189549
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scoring Script"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 'batch-pipeline/score.py'\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "from azureml.core import Model\r\n",
        "from pyspark.ml import PipelineModel\r\n",
        "#from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "\r\n",
        "def init():\r\n",
        "    global model\r\n",
        "    #spark = SparkSession.builder.getOrCreate()\r\n",
        "    model_path = Model.get_model_path('pyspark-batch-leads-model')\r\n",
        "    model= PipelineModel.load(model_path)\r\n",
        "\r\n",
        "def run(mini_batch):\r\n",
        "    # This runs for each batch\r\n",
        "    resultList = []\r\n",
        "    # process each file in the batch\r\n",
        "    for f in mini_batch:\r\n",
        "        df = spark.read.csv(path=f,header=\"true\",inferSchema=\"true\",sep=\",\").drop('_c0')\r\n",
        "        prediction = model.transform(df).select('prediction').toPandas().prediction.map({0.0:\"no\",1.0:\"yes\"}).to_numpy()\r\n",
        "        resultList.append(\"{}: {}\".format(os.path.basename(f), prediction))\r\n",
        "    return resultList\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting batch-pipeline/score.py\n"
          ]
        }
      ],
      "execution_count": 37,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625970033791
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\r\n",
        "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\r\n",
        "\r\n",
        "# Create an Environment for the experiment\r\n",
        "batch_env = Environment.from_conda_specification(name=\"experiment_env\", file_path=\"batch-pipeline/batch_environment.yml\")\r\n",
        "batch_env.docker.base_image = DEFAULT_CPU_IMAGE\r\n",
        "print('Configuration ready.')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration ready.\n"
          ]
        }
      ],
      "execution_count": 38,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625980362311
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\r\n",
        "from azureml.data import OutputFileDatasetConfig\r\n",
        "from azureml.core.runconfig import DockerConfiguration\r\n",
        "\r\n",
        "output_dir = OutputFileDatasetConfig(name='inferences')\r\n",
        "\r\n",
        "parallel_run_config = ParallelRunConfig(\r\n",
        "    source_directory='batch-pipeline/',\r\n",
        "    entry_script=\"score.py\",\r\n",
        "    mini_batch_size=\"5\",\r\n",
        "    error_threshold=10,\r\n",
        "    output_action=\"append_row\",\r\n",
        "    environment=batch_env,\r\n",
        "    compute_target=inference_cluster,\r\n",
        "    node_count=2)\r\n",
        "\r\n",
        "parallelrun_step = ParallelRunStep(\r\n",
        "    name='batch-score-leads',\r\n",
        "    parallel_run_config=parallel_run_config,\r\n",
        "    inputs=[batch_data_set.as_named_input('leads_batch')],\r\n",
        "    output=output_dir,\r\n",
        "    arguments=[],\r\n",
        "    allow_reuse=True\r\n",
        ")\r\n",
        "\r\n",
        "print('Steps defined')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps defined\n"
          ]
        }
      ],
      "execution_count": 39,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625980364309
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\r\n",
        "from azureml.pipeline.core import Pipeline\r\n",
        "\r\n",
        "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\r\n",
        "pipeline_run = Experiment(workspace=ws, name='leads-batch-pipeline').submit(pipeline)\r\n",
        "pipeline_run.wait_for_completion(show_output=True)\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created step batch-score-leads [00aced3a][a2719f31-7cef-4639-b528-3b338977fd20], (This step will run and generate new outputs)\n",
            "Submitted PipelineRun 6277edd5-3538-4ea1-94c3-8df36492360a\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/6277edd5-3538-4ea1-94c3-8df36492360a?wsid=/subscriptions/23416925-66df-470c-b651-f378856d8ad7/resourcegroups/rohan-rg/workspaces/rohan-ws&tid=13715ad3-e049-4909-899b-f9e22f99b1a5\n",
            "PipelineRunId: 6277edd5-3538-4ea1-94c3-8df36492360a\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/6277edd5-3538-4ea1-94c3-8df36492360a?wsid=/subscriptions/23416925-66df-470c-b651-f378856d8ad7/resourcegroups/rohan-rg/workspaces/rohan-ws&tid=13715ad3-e049-4909-899b-f9e22f99b1a5\n",
            "PipelineRun Status: NotStarted\n",
            "PipelineRun Status: Running\n",
            "\n",
            "\n",
            "StepRunId: dfdb9062-eceb-4862-a2a3-519526f975e5\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/dfdb9062-eceb-4862-a2a3-519526f975e5?wsid=/subscriptions/23416925-66df-470c-b651-f378856d8ad7/resourcegroups/rohan-rg/workspaces/rohan-ws&tid=13715ad3-e049-4909-899b-f9e22f99b1a5\n",
            "StepRun( batch-score-leads ) Status: NotStarted\n",
            "StepRun( batch-score-leads ) Status: Running\n",
            "\n",
            "Streaming azureml-logs/55_azureml-execution-tvmps_6e845d0ae9daa0718fc6542751b650e6ef4f6dbbbe64579dbef32724cf7dd364_d.txt\n",
            "========================================================================================================================\n",
            "2021-07-11T05:15:43Z Running following command: /bin/bash -c sudo blobfuse /mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/mounts/workspaceblobstore --tmp-path=/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/caches/workspaceblobstore -o ro --file-cache-timeout-in-seconds=1000000 --cache-size-mb=24790 -o nonempty -o allow_other --config-file=/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/configs/workspaceblobstore.cfg --log-level=LOG_WARNING\n",
            "2021-07-11T05:15:43Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/mounts/workspaceblobstore\n",
            "2021-07-11T05:15:43Z The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
            "2021-07-11T05:15:43Z Starting output-watcher...\n",
            "2021-07-11T05:15:43Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
            "Login Succeeded\n",
            "Using default tag: latest\n",
            "latest: Pulling from azureml/azureml_fe4afc798de401edfb76dc27a38b1703\n",
            "92473f7ef455: Pulling fs layer\n",
            "fb52bde70123: Pulling fs layer\n",
            "64788f86be3f: Pulling fs layer\n",
            "33f6d5f2e001: Pulling fs layer\n",
            "eeb715f1b6ae: Pulling fs layer\n",
            "fe519cf36537: Pulling fs layer\n",
            "58ff99196c15: Pulling fs layer\n",
            "9b13f06a8eff: Pulling fs layer\n",
            "2d4e93adbf58: Pulling fs layer\n",
            "6ee7c3767844: Pulling fs layer\n",
            "62cfc3ccb8ab: Pulling fs layer\n",
            "33f6d5f2e001: Waiting\n",
            "eeb715f1b6ae: Waiting\n",
            "fe519cf36537: Waiting\n",
            "58ff99196c15: Waiting\n",
            "9b13f06a8eff: Waiting\n",
            "2d4e93adbf58: Waiting\n",
            "6ee7c3767844: Waiting\n",
            "4a7af9d757ee: Pulling fs layer\n",
            "9e11d437728f: Pulling fs layer\n",
            "3506c910620f: Pulling fs layer\n",
            "afe6352c52c2: Pulling fs layer\n",
            "45d886309004: Pulling fs layer\n",
            "2ce19e789040: Pulling fs layer\n",
            "f2a2950e1ed4: Pulling fs layer\n",
            "62cfc3ccb8ab: Waiting\n",
            "9e11d437728f: Waiting\n",
            "3506c910620f: Waiting\n",
            "afe6352c52c2: Waiting\n",
            "45d886309004: Waiting\n",
            "2ce19e789040: Waiting\n",
            "f2a2950e1ed4: Waiting\n",
            "4a7af9d757ee: Waiting\n",
            "64788f86be3f: Verifying Checksum\n",
            "64788f86be3f: Download complete\n",
            "fb52bde70123: Verifying Checksum\n",
            "fb52bde70123: Download complete\n",
            "33f6d5f2e001: Verifying Checksum\n",
            "33f6d5f2e001: Download complete\n",
            "fe519cf36537: Verifying Checksum\n",
            "fe519cf36537: Download complete\n",
            "92473f7ef455: Verifying Checksum\n",
            "92473f7ef455: Download complete\n",
            "58ff99196c15: Verifying Checksum\n",
            "58ff99196c15: Download complete\n",
            "eeb715f1b6ae: Verifying Checksum\n",
            "eeb715f1b6ae: Download complete\n",
            "9b13f06a8eff: Verifying Checksum\n",
            "9b13f06a8eff: Download complete\n",
            "6ee7c3767844: Verifying Checksum\n",
            "6ee7c3767844: Download complete\n",
            "62cfc3ccb8ab: Verifying Checksum\n",
            "62cfc3ccb8ab: Download complete\n",
            "4a7af9d757ee: Verifying Checksum\n",
            "4a7af9d757ee: Download complete\n",
            "2d4e93adbf58: Verifying Checksum\n",
            "2d4e93adbf58: Download complete\n",
            "9e11d437728f: Verifying Checksum\n",
            "9e11d437728f: Download complete\n",
            "45d886309004: Verifying Checksum\n",
            "45d886309004: Download complete\n",
            "afe6352c52c2: Verifying Checksum\n",
            "afe6352c52c2: Download complete\n",
            "2ce19e789040: Verifying Checksum\n",
            "2ce19e789040: Download complete\n",
            "f2a2950e1ed4: Verifying Checksum\n",
            "f2a2950e1ed4: Download complete\n",
            "3506c910620f: Verifying Checksum\n",
            "3506c910620f: Download complete\n",
            "92473f7ef455: Pull complete\n",
            "fb52bde70123: Pull complete\n",
            "64788f86be3f: Pull complete\n",
            "33f6d5f2e001: Pull complete\n",
            "eeb715f1b6ae: Pull complete\n",
            "fe519cf36537: Pull complete\n",
            "58ff99196c15: Pull complete\n",
            "9b13f06a8eff: Pull complete\n",
            "2d4e93adbf58: Pull complete\n",
            "6ee7c3767844: Pull complete\n",
            "62cfc3ccb8ab: Pull complete\n",
            "4a7af9d757ee: Pull complete\n",
            "9e11d437728f: Pull complete\n",
            "3506c910620f: Pull complete\n",
            "afe6352c52c2: Pull complete\n",
            "45d886309004: Pull complete\n",
            "2ce19e789040: Pull complete\n",
            "f2a2950e1ed4: Pull complete\n",
            "Digest: sha256:5224cd9c4e07c9304c90193ab084da3cf8643e81065eef4d81c2c4029c58248c\n",
            "Status: Downloaded newer image for viennaglobal.azurecr.io/azureml/azureml_fe4afc798de401edfb76dc27a38b1703:latest\n",
            "viennaglobal.azurecr.io/azureml/azureml_fe4afc798de401edfb76dc27a38b1703:latest\n",
            "2021-07-11T05:16:08Z The vmsize standard_ds11_v2 is not a GPU VM, skipping get GPU count by running nvidia-smi command.\n",
            "2021-07-11T05:16:08Z Check if container dfdb9062-eceb-4862-a2a3-519526f975e5_DataSidecar already exist exited with 0, \n",
            "\n",
            "999298b19986e48ec5db49a6c8ad0eba2e815d5c8d43963541524ec4472dad2b\n",
            "2021-07-11T05:16:12Z Parameters for containerSetup task: useDetonationChamer set to false and sshRequired set to false \n",
            "2021-07-11T05:16:12Z containerSetup task cmd: [/mnt/batch/tasks/startup/wd/hosttools -task=containerSetup -traceContext=00-02b939f6533a3e1d7e4994832112679a-8b0c068a5edecaa3-01 -sshRequired=false] \n",
            "2021/07/11 05:16:12 Starting App Insight Logger for task:  containerSetup\n",
            "2021/07/11 05:16:12 Version: 3.0.01632.0003 Branch: .SourceBranch Commit: 4b96fb0\n",
            "2021/07/11 05:16:12 Entered ContainerSetupTask - Preparing infiniband\n",
            "2021/07/11 05:16:12 Starting infiniband setup\n",
            "2021/07/11 05:16:12 Python Version found is Python 3.7.9\n",
            "\n",
            "2021/07/11 05:16:12 Returning Python Version as 3.7\n",
            "2021/07/11 05:16:12 VMSize: standard_ds11_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
            "2021/07/11 05:16:12 VMSize: standard_ds11_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
            "2021-07-11T05:16:12Z VMSize: standard_ds11_v2, Host: runtime-gen1-ubuntu18, Container: ubuntu-16.04\n",
            "2021/07/11 05:16:12 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
            "2021/07/11 05:16:12 Not setting up Infiniband in Container\n",
            "2021/07/11 05:16:12 Not setting up Infiniband in Container\n",
            "2021-07-11T05:16:12Z Not setting up Infiniband in Container\n",
            "2021/07/11 05:16:12 Python Version found is Python 3.7.9\n",
            "\n",
            "2021/07/11 05:16:12 Returning Python Version as 3.7\n",
            "2021/07/11 05:16:12 sshd inside container not required for job, skipping setup.\n",
            "2021/07/11 05:16:13 All App Insights Logs was sent successfully or the close timeout of 20 was reached\n",
            "2021/07/11 05:16:13 App Insight Client has already been closed\n",
            "2021/07/11 05:16:13 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
            "Stopped: false\n",
            "OriginalData: 1\n",
            "FilteredData: 0.\n",
            "2021-07-11T05:16:13Z Starting docker container succeeded.\n",
            "\n",
            "Streaming azureml-logs/65_job_prep-tvmps_99842464acbdaf5276ddd6e085caeed8142da7492081774032bb851aa993426b_d.txt\n",
            "===============================================================================================================\n",
            "[2021-07-11T05:16:14.940013] Entering job preparation.\n",
            "[2021-07-11T05:16:15.733136] Starting job preparation.\n",
            "[2021-07-11T05:16:15.733173] Extracting the control code.\n",
            "[2021-07-11T05:16:15.733834] Starting extract_project.\n",
            "[2021-07-11T05:16:15.733973] Starting to extract zip file.\n",
            "[2021-07-11T05:16:15.766974] Finished extracting zip file.\n",
            "[2021-07-11T05:16:15.769833] Using urllib.request Python 3.0 or later\n",
            "[2021-07-11T05:16:15.769934] Start fetching snapshots.\n",
            "[2021-07-11T05:16:15.770167] Start fetching snapshot.\n",
            "[2021-07-11T05:16:15.770255] Retrieving project from snapshot: e729a46b-c476-41d4-a5f5-53d54f849e4c\n",
            "Starting the daemon thread to refresh tokens in background for process with pid = 41\n",
            "[2021-07-11T05:16:16.179226] Finished fetching snapshot.\n",
            "[2021-07-11T05:16:16.179281] Start fetching snapshot.\n",
            "[2021-07-11T05:16:16.179291] Retrieving project from snapshot: d0c57f8d-9f67-42a2-8634-16a49782dd43\n",
            "[2021-07-11T05:16:24.672182] Finished fetching snapshot.\n",
            "[2021-07-11T05:16:24.672236] Finished fetching snapshots.\n",
            "[2021-07-11T05:16:24.672261] Finished extract_project.\n",
            "[2021-07-11T05:16:24.672559] Finished fetching and extracting the control code.\n",
            "[2021-07-11T05:16:24.680411] Start run_history_prep.\n",
            "[2021-07-11T05:16:24.687317] Job preparation is complete.\n",
            "[2021-07-11T05:16:24.687614] Entering Data Context Managers in Sidecar\n",
            "[2021-07-11T05:16:24.688543] Running Sidecar prep cmd...\n",
            "[2021-07-11T05:16:25.070921] INFO azureml.sidecar.sidecar: Received task: enter_contexts. Running on Linux at /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5\n",
            "[2021-07-11T05:16:25.072045] INFO azureml.sidecar.sidecar: Invoking \"enter_contexts\" task with Context Managers: {\"context_managers\": [\"Dataset:context_managers.Datasets\"]}\n",
            "Enter __enter__ of DatasetContextManager\n",
            "SDK version: azureml-core==1.28.0 azureml-dataprep==2.16.0. Session id: f818ed31-5ef0-4827-89cd-3023dacd9a8c. Run id: dfdb9062-eceb-4862-a2a3-519526f975e5.\n",
            "Processing 'leads_batch'.\n",
            "Processing dataset FileDataset\n",
            "{\n",
            "  \"source\": [\n",
            "    \"('rohands', 'batch-data/')\"\n",
            "  ],\n",
            "  \"definition\": [\n",
            "    \"GetDatastoreFiles\"\n",
            "  ],\n",
            "  \"registration\": {\n",
            "    \"id\": \"c73804df-9317-415e-b2ed-32de72b2948b\",\n",
            "    \"name\": \"leads-batch-data\",\n",
            "    \"version\": 1,\n",
            "    \"description\": \"batch data for Marketing Leads UCI\",\n",
            "    \"workspace\": \"Workspace.create(name='rohan-ws', subscription_id='23416925-66df-470c-b651-f378856d8ad7', resource_group='rohan-rg')\"\n",
            "  }\n",
            "}\n",
            "Mounting leads_batch to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/leads_batch_c73804df-9317-415e-b2ed-32de72b2948b.\n",
            "Mounted leads_batch to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/leads_batch_c73804df-9317-415e-b2ed-32de72b2948b as folder.\n",
            "Processing 'inferences'.\n",
            "Mounted inferences to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/inferences_workspaceblobstore.\n",
            "Exit __enter__ of DatasetContextManager\n",
            "Set Dataset leads_batch's target path to /mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/leads_batch_c73804df-9317-415e-b2ed-32de72b2948b\n",
            "Set OutputDataset inferences's target path to /tmp/499f23ce-6ee6-475d-9389-1ba6221d34ac\n",
            "[2021-07-11T05:16:41.458596] INFO azureml.sidecar.task.enter_contexts: Entered Context Managers\n",
            "[2021-07-11T05:16:41.902233] Ran Sidecar prep cmd.\n",
            "[2021-07-11T05:16:41.902375] Running Context Managers in Sidecar complete.\n",
            "\n",
            "Streaming azureml-logs/65_job_prep-tvmps_6e845d0ae9daa0718fc6542751b650e6ef4f6dbbbe64579dbef32724cf7dd364_d.txt\n",
            "===============================================================================================================\n",
            "[2021-07-11T05:16:13.963935] Entering job preparation.\n",
            "[2021-07-11T05:16:14.722283] Starting job preparation.\n",
            "[2021-07-11T05:16:14.722751] Extracting the control code.\n",
            "[2021-07-11T05:16:14.723418] Starting extract_project.\n",
            "[2021-07-11T05:16:14.723744] Starting to extract zip file.\n",
            "[2021-07-11T05:16:14.758394] Finished extracting zip file.\n",
            "[2021-07-11T05:16:14.760937] Using urllib.request Python 3.0 or later\n",
            "[2021-07-11T05:16:14.761249] Start fetching snapshots.\n",
            "[2021-07-11T05:16:14.761662] Start fetching snapshot.\n",
            "[2021-07-11T05:16:14.762083] Retrieving project from snapshot: e729a46b-c476-41d4-a5f5-53d54f849e4c\n",
            "Starting the daemon thread to refresh tokens in background for process with pid = 40\n",
            "[2021-07-11T05:16:17.077646] Finished fetching snapshot.\n",
            "[2021-07-11T05:16:17.077678] Start fetching snapshot.\n",
            "[2021-07-11T05:16:17.077706] Retrieving project from snapshot: d0c57f8d-9f67-42a2-8634-16a49782dd43\n",
            "[2021-07-11T05:16:24.727627] Finished fetching snapshot.\n",
            "[2021-07-11T05:16:24.727659] Finished fetching snapshots.\n",
            "[2021-07-11T05:16:24.727667] Finished extract_project.\n",
            "[2021-07-11T05:16:24.727768] Finished fetching and extracting the control code.\n",
            "[2021-07-11T05:16:24.734638] Start run_history_prep.\n",
            "[2021-07-11T05:16:24.741164] Job preparation is complete.\n",
            "[2021-07-11T05:16:24.741353] Entering Data Context Managers in Sidecar\n",
            "[2021-07-11T05:16:24.749075] Running Sidecar prep cmd...\n",
            "[2021-07-11T05:16:25.115253] INFO azureml.sidecar.sidecar: Received task: enter_contexts. Running on Linux at /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5\n",
            "[2021-07-11T05:16:25.116277] INFO azureml.sidecar.sidecar: Invoking \"enter_contexts\" task with Context Managers: {\"context_managers\": [\"Dataset:context_managers.Datasets\"]}\n",
            "Enter __enter__ of DatasetContextManager\n",
            "SDK version: azureml-core==1.28.0 azureml-dataprep==2.16.0. Session id: 0f11c654-d4e3-4685-96d7-36900b6c9754. Run id: dfdb9062-eceb-4862-a2a3-519526f975e5.\n",
            "Processing 'leads_batch'.\n",
            "Processing dataset FileDataset\n",
            "{\n",
            "  \"source\": [\n",
            "    \"('rohands', 'batch-data/')\"\n",
            "  ],\n",
            "  \"definition\": [\n",
            "    \"GetDatastoreFiles\"\n",
            "  ],\n",
            "  \"registration\": {\n",
            "    \"id\": \"c73804df-9317-415e-b2ed-32de72b2948b\",\n",
            "    \"name\": \"leads-batch-data\",\n",
            "    \"version\": 1,\n",
            "    \"description\": \"batch data for Marketing Leads UCI\",\n",
            "    \"workspace\": \"Workspace.create(name='rohan-ws', subscription_id='23416925-66df-470c-b651-f378856d8ad7', resource_group='rohan-rg')\"\n",
            "  }\n",
            "}\n",
            "Mounting leads_batch to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/leads_batch_c73804df-9317-415e-b2ed-32de72b2948b.\n",
            "Mounted leads_batch to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/leads_batch_c73804df-9317-415e-b2ed-32de72b2948b as folder.\n",
            "Processing 'inferences'.\n",
            "Mounted inferences to /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/inferences_workspaceblobstore.\n",
            "Exit __enter__ of DatasetContextManager\n",
            "Set Dataset leads_batch's target path to /mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/leads_batch_c73804df-9317-415e-b2ed-32de72b2948b\n",
            "Set OutputDataset inferences's target path to /tmp/499f23ce-6ee6-475d-9389-1ba6221d34ac\n",
            "[2021-07-11T05:16:41.537677] INFO azureml.sidecar.task.enter_contexts: Entered Context Managers\n",
            "[2021-07-11T05:16:41.869469] Ran Sidecar prep cmd.\n",
            "[2021-07-11T05:16:41.869719] Running Context Managers in Sidecar complete.\n",
            "\n",
            "Streaming azureml-logs/70_driver_log.txt\n",
            "========================================\n",
            "2021/07/11 05:17:33 Starting App Insight Logger for task:  runTaskLet\n",
            "2021/07/11 05:17:33 Version: 3.0.01632.0003 Branch: .SourceBranch Commit: 4b96fb0\n",
            "2021/07/11 05:17:33 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/info\n",
            "2021/07/11 05:17:33 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/status\n",
            "[2021-07-11T05:17:33.056883] Entering context manager injector.\n",
            "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'Dataset:context_managers.Datasets', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['driver/amlbi_main.py', '--client_sdk_version', '1.32.0', '--scoring_module_name', 'score.py', '--mini_batch_size', '5', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--output', 'DatasetOutputConfig:inferences', '--input_fds_0', 'leads_batch'])\n",
            "Script type = None\n",
            "[2021-07-11T05:17:33.606672] Entering Run History Context Manager.\n",
            "[2021-07-11T05:17:37.256930] Current directory: /mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5\n",
            "[2021-07-11T05:17:37.256978] Preparing to call script [driver/amlbi_main.py] with arguments:['--client_sdk_version', '1.32.0', '--scoring_module_name', 'score.py', '--mini_batch_size', '5', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--output', '$inferences', '--input_fds_0', 'leads_batch']\n",
            "[2021-07-11T05:17:37.257007] After variable expansion, calling script [driver/amlbi_main.py] with arguments:['--client_sdk_version', '1.32.0', '--scoring_module_name', 'score.py', '--mini_batch_size', '5', '--error_threshold', '10', '--output_action', 'append_row', '--logging_level', 'INFO', '--run_invocation_timeout', '60', '--run_max_try', '3', '--create_snapshot_at_runtime', 'True', '--output', '/tmp/499f23ce-6ee6-475d-9389-1ba6221d34ac', '--input_fds_0', 'leads_batch']\n",
            "\n",
            "2021/07/11 05:17:37 Not exporting to RunHistory as the exporter is either stopped or there is no data.\n",
            "Stopped: false\n",
            "OriginalData: 1\n",
            "FilteredData: 0.\n",
            "\n",
            "\n",
            "[2021-07-11T05:20:11.150048] The experiment failed. Finalizing run...\n",
            "Cleaning up all outstanding Run operations, waiting 900.0 seconds\n",
            "3 items cleaning up...\n",
            "Cleanup took 0.16404509544372559 seconds\n",
            "azureml_common.parallel_run.exception_info.Exception: Run failed. Below is the error detail:\n",
            "EntryScriptException: Entry script error. All tries to load the entry script or calling init() failed. Please check logs/user/error/* and logs/sys/error/* to see if some errors have occurred.\n",
            "No mini batch has been completed. Consider a succeeded mini batch or failed mini batch reached the max tries as completed.\n",
            "The init() function in the entry script had raised exception for 13 times. Please check logs at logs/user/error/* for details.\n",
            "  * Error 'Java gateway process exited before sending its port number' occurred 26 times.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"driver/amlbi_main.py\", line 167, in <module>\n",
            "    main()\n",
            "  File \"driver/amlbi_main.py\", line 123, in main\n",
            "    boot(driver_dir)\n",
            "  File \"driver/amlbi_main.py\", line 58, in boot\n",
            "    booter.start()\n",
            "  File \"driver/azureml_user/parallel_run/boot.py\", line 361, in start\n",
            "    self.start_sys_main()\n",
            "  File \"driver/azureml_user/parallel_run/boot.py\", line 259, in start_sys_main\n",
            "    self.run_sys_main(cmd)\n",
            "  File \"driver/azureml_user/parallel_run/boot_simulator.py\", line 40, in run_sys_main\n",
            "    self.run(cmd)\n",
            "  File \"driver/azureml_user/parallel_run/boot.py\", line 201, in run\n",
            "    self.check_run_result(proc=proc, stdout=stdout, stderr=stderr)\n",
            "  File \"driver/azureml_user/parallel_run/boot.py\", line 211, in check_run_result\n",
            "    BootResult().check_result(stdout)\n",
            "  File \"driver/azureml_user/parallel_run/boot_result.py\", line 36, in check_result\n",
            "    raise Exception(message) from cause\n",
            "Exception: Run failed, please check logs for details. You can check logs/readme.txt for the layout of logs.\n",
            "\n",
            "[2021-07-11T05:20:11.449576] Finished context manager injector with Exception.\n",
            "\n",
            "Streaming azureml-logs/75_job_post-tvmps_99842464acbdaf5276ddd6e085caeed8142da7492081774032bb851aa993426b_d.txt\n",
            "===============================================================================================================\n",
            "[2021-07-11T05:20:16.604433] Entering job release\n",
            "[2021-07-11T05:20:17.490889] Starting job release\n",
            "[2021-07-11T05:20:17.491426] Logging experiment finalizing status in history service.\n",
            "Starting the daemon thread to refresh tokens in background for process with pid = 387\n",
            "[2021-07-11T05:20:17.492283] job release stage : upload_datastore starting...\n",
            "[2021-07-11T05:20:17.505576] Entering context manager injector.\n",
            "[2021-07-11T05:20:17.508795] job release stage : upload_datastore completed...\n",
            "[2021-07-11T05:20:17.509145] job release stage : start importing azureml.history._tracking in run_history_release.\n",
            "[2021-07-11T05:20:17.509173] job release stage : execute_job_release starting...\n",
            "[2021-07-11T05:20:17.509708] job release stage : copy_batchai_cached_logs starting...\n",
            "[2021-07-11T05:20:17.509982] job release stage : copy_batchai_cached_logs completed...\n",
            "[2021-07-11T05:20:17.591862] job release stage : send_run_telemetry starting...\n",
            "[2021-07-11T05:20:17.608030] get vm size and vm region successfully.\n",
            "[2021-07-11T05:20:17.615110] get compute meta data successfully.\n",
            "[2021-07-11T05:20:17.634450] job release stage : execute_job_release completed...\n",
            "[2021-07-11T05:20:17.920801] post artifact meta request successfully.\n",
            "[2021-07-11T05:20:17.953126] upload compute record artifact successfully.\n",
            "[2021-07-11T05:20:17.953206] job release stage : send_run_telemetry completed...\n",
            "[2021-07-11T05:20:17.953796] Running in AzureML-Sidecar, starting to exit user context managers...\n",
            "[2021-07-11T05:20:17.953916] Running Sidecar release cmd...\n",
            "[2021-07-11T05:20:18.042524] INFO azureml.sidecar.sidecar: Received task: exit_contexts. Running on Linux at /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5\n",
            "Enter __exit__ of DatasetContextManager\n",
            "Unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/leads_batch_c73804df-9317-415e-b2ed-32de72b2948b.\n",
            "Finishing unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/leads_batch_c73804df-9317-415e-b2ed-32de72b2948b.\n",
            "Unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/inferences_workspaceblobstore.\n",
            "Finishing unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/inferences_workspaceblobstore.\n",
            "Exit __exit__ of DatasetContextManager\n",
            "[2021-07-11T05:20:18.097479] Removing absolute paths from host...\n",
            "[2021-07-11T05:20:18.097772] INFO azureml.sidecar.task.exit_contexts: Exited Context Managers\n",
            "[2021-07-11T05:20:18.427627] Ran Sidecar release cmd.\n",
            "[2021-07-11T05:20:18.427876] Job release is complete\n",
            "\n",
            "Streaming azureml-logs/75_job_post-tvmps_6e845d0ae9daa0718fc6542751b650e6ef4f6dbbbe64579dbef32724cf7dd364_d.txt\n",
            "===============================================================================================================\n",
            "[2021-07-11T05:20:17.262542] Entering job release\n",
            "[2021-07-11T05:20:18.103332] job release stage : copy_batchai_cached_logs starting...\n",
            "[2021-07-11T05:20:18.103374] job release stage : copy_batchai_cached_logs completed...\n",
            "[2021-07-11T05:20:18.103581] Running in AzureML-Sidecar, starting to exit user context managers...\n",
            "[2021-07-11T05:20:18.104115] Running Sidecar release cmd...\n",
            "[2021-07-11T05:20:18.176401] INFO azureml.sidecar.sidecar: Received task: exit_contexts. Running on Linux at /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5\n",
            "Enter __exit__ of DatasetContextManager\n",
            "Unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/leads_batch_c73804df-9317-415e-b2ed-32de72b2948b.\n",
            "Finishing unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/leads_batch_c73804df-9317-415e-b2ed-32de72b2948b.\n",
            "Unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/inferences_workspaceblobstore.\n",
            "Finishing unmounting /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/rohan-ws/azureml/dfdb9062-eceb-4862-a2a3-519526f975e5/wd/inferences_workspaceblobstore.\n",
            "Exit __exit__ of DatasetContextManager\n",
            "[2021-07-11T05:20:18.217402] Removing absolute paths from host...\n",
            "[2021-07-11T05:20:18.217589] INFO azureml.sidecar.task.exit_contexts: Exited Context Managers\n",
            "[2021-07-11T05:20:19.023299] Ran Sidecar release cmd.\n",
            "\n",
            "StepRun(batch-score-leads) Execution Summary\n",
            "=============================================\n",
            "StepRun( batch-score-leads ) Status: Failed\n",
            "\n",
            "Warnings:\n",
            "{\n",
            "  \"error\": {\n",
            "    \"code\": \"UserError\",\n",
            "    \"severity\": null,\n",
            "    \"message\": \"AzureMLCompute job failed.\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\n\\tReason: Job failed with non-zero exit Code\",\n",
            "    \"messageFormat\": \"{Message}\",\n",
            "    \"messageParameters\": {\n",
            "      \"Message\": \"AzureMLCompute job failed.\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\n\\tReason: Job failed with non-zero exit Code\"\n",
            "    },\n",
            "    \"referenceCode\": null,\n",
            "    \"detailsUri\": null,\n",
            "    \"target\": null,\n",
            "    \"details\": [],\n",
            "    \"innerError\": {\n",
            "      \"code\": \"UserTrainingScriptFailed\",\n",
            "      \"innerError\": null\n",
            "    },\n",
            "    \"debugInfo\": null,\n",
            "    \"additionalInfo\": null\n",
            "  },\n",
            "  \"correlation\": {\n",
            "    \"operation\": \"91782c797d07e04f937b645b0a84f4fd\",\n",
            "    \"request\": \"1d91b3886c1b03c9\"\n",
            "  },\n",
            "  \"environment\": \"centralindia\",\n",
            "  \"location\": \"centralindia\",\n",
            "  \"time\": \"2021-07-11T05:20:36.5110505+00:00\",\n",
            "  \"componentName\": \"execution-worker\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ActivityFailedException",
          "evalue": "ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"User program failed with Exception: Run failed, please check logs for details. You can check logs/readme.txt for the layout of logs.\",\n        \"messageParameters\": {},\n        \"detailsUri\": \"https://aka.ms/azureml-run-troubleshooting\",\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"User program failed with Exception: Run failed, please check logs for details. You can check logs/readme.txt for the layout of logs.\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"detailsUri\\\": \\\"https://aka.ms/azureml-run-troubleshooting\\\",\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-a30e2517756a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparallelrun_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpipeline_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'leads-batch-pipeline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpipeline_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    293\u001b[0m                             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                                 step_run.wait_for_completion(timeout_seconds=timeout_seconds - time_elapsed,\n\u001b[0;32m--> 295\u001b[0;31m                                                              raise_on_error=raise_on_error)\n\u001b[0m\u001b[1;32m    296\u001b[0m                             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                                 \u001b[0;31m# If there are package conflicts in the user's environment, the run rehydration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 return self._stream_run_output(timeout_seconds=timeout_seconds,\n\u001b[0;32m--> 737\u001b[0;31m                                                raise_on_error=raise_on_error)\n\u001b[0m\u001b[1;32m    738\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                 \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The output streaming for the run interrupted.\\n\"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36m_stream_run_output\u001b[0;34m(self, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mActivityFailedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_details\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_details\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mActivityFailedException\u001b[0m: ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"User program failed with Exception: Run failed, please check logs for details. You can check logs/readme.txt for the layout of logs.\",\n        \"messageParameters\": {},\n        \"detailsUri\": \"https://aka.ms/azureml-run-troubleshooting\",\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"User program failed with Exception: Run failed, please check logs for details. You can check logs/readme.txt for the layout of logs.\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"detailsUri\\\": \\\"https://aka.ms/azureml-run-troubleshooting\\\",\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}"
          ]
        }
      ],
      "execution_count": 40,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "published_pipeline = pipeline_run.publish_pipeline(name='leads-batch-pipeline', description='Batch scoring of leads data from UCI', version='1.0')\r\n",
        "rest_endpoint = published_pipeline.endpoint\r\n",
        "print(rest_endpoint)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the Pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.authentication import InteractiveLoginAuthentication\r\n",
        "from azureml.pipeline.core.run import PipelineRun\r\n",
        "from azureml.widgets import RunDetails\r\n",
        "import requests\r\n",
        "\r\n",
        "interactive_auth = InteractiveLoginAuthentication()\r\n",
        "auth_header = interactive_auth.get_authentication_header()\r\n",
        "\r\n",
        "rest_endpoint = published_pipeline.endpoint\r\n",
        "response = requests.post(rest_endpoint, headers=auth_header, json={\"ExperimentName\": \"leads-batch-pipeline\"})\r\n",
        "run_id = response.json()[\"Id\"]\r\n",
        "\r\n",
        "published_pipeline_run = PipelineRun(ws.experiments['leads-batch-pipeline'], run_id)\r\n",
        "published_pipeline_run.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\r\n",
        ".addGrid(gbt.maxDepth,[5,10])\r\n",
        ".addGrid(gbt.maxIter,[10,50])\r\n",
        ".addGrid(gbt.maxBins,[16,32])\r\n",
        ".addGrid(gbt.stepSize,[0.05,0.1])\r\n",
        "'''"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1625970416665
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}